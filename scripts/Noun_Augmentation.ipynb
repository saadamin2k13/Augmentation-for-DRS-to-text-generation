{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5e3285b57425419fbad3c3c711286936": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_828a9d20e65f4e0795b162428868a046",
              "IPY_MODEL_123fee40798f4221a7588c4f04408615",
              "IPY_MODEL_954ccd7a9e244a6dbf748e918619c98f"
            ],
            "layout": "IPY_MODEL_a0b5cd94fb2748af953123e64667afee"
          }
        },
        "828a9d20e65f4e0795b162428868a046": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6fea85f894a34d3580671f3edf6e95eb",
            "placeholder": "​",
            "style": "IPY_MODEL_c3c16bded96a4b5f8a27908c4ccfc77b",
            "value": "Downloading (…)solve/main/vocab.txt: 100%"
          }
        },
        "123fee40798f4221a7588c4f04408615": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12d8043f00234b7f96618fe9a2ec064c",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_88e8297e8cba4c0a9d4dc22592dc312a",
            "value": 231508
          }
        },
        "954ccd7a9e244a6dbf748e918619c98f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f16914f6724b45c3b51308f94a739275",
            "placeholder": "​",
            "style": "IPY_MODEL_f8abcb58a8a84417afc8fdb0323a2241",
            "value": " 232k/232k [00:00&lt;00:00, 2.25MB/s]"
          }
        },
        "a0b5cd94fb2748af953123e64667afee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6fea85f894a34d3580671f3edf6e95eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3c16bded96a4b5f8a27908c4ccfc77b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "12d8043f00234b7f96618fe9a2ec064c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88e8297e8cba4c0a9d4dc22592dc312a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f16914f6724b45c3b51308f94a739275": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8abcb58a8a84417afc8fdb0323a2241": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "51329aaa77cf44f49ab2eddc36490eb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_934630af56cf45478fd6c05880f80d2e",
              "IPY_MODEL_b051da4e9a314dfe860594468f51f3d3",
              "IPY_MODEL_6a2a68352e2047a6a4c7b3faf6c9a996"
            ],
            "layout": "IPY_MODEL_0c61ab1c203e4dd287bdd3ca29bd7edc"
          }
        },
        "934630af56cf45478fd6c05880f80d2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf14b71f17f54b87afcd0ead9979e70a",
            "placeholder": "​",
            "style": "IPY_MODEL_7b2769403d8242c5b78eee617e317b5d",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "b051da4e9a314dfe860594468f51f3d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4fa7912822c0449386836f9730a6d07b",
            "max": 384,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5c5cbe4078bf41229aa0b07c12e90c03",
            "value": 384
          }
        },
        "6a2a68352e2047a6a4c7b3faf6c9a996": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_223943bf86c14f9dae7f8f3b5381d3bd",
            "placeholder": "​",
            "style": "IPY_MODEL_f33e68d382834bcf913d89811bd04a00",
            "value": " 384/384 [00:00&lt;00:00, 19.0kB/s]"
          }
        },
        "0c61ab1c203e4dd287bdd3ca29bd7edc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf14b71f17f54b87afcd0ead9979e70a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b2769403d8242c5b78eee617e317b5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4fa7912822c0449386836f9730a6d07b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c5cbe4078bf41229aa0b07c12e90c03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "223943bf86c14f9dae7f8f3b5381d3bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f33e68d382834bcf913d89811bd04a00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "279791ca2aa44967890eba86b1fb5a99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3c18d16c634f486693b158dd41af3240",
              "IPY_MODEL_7e5f14d2355f4246bb920a36e4a6bc18",
              "IPY_MODEL_97b8438f0ef3476088dbe500c2d3481a"
            ],
            "layout": "IPY_MODEL_56c4cd598af04da3aeffb2987b51153e"
          }
        },
        "3c18d16c634f486693b158dd41af3240": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c16d6f445da04b04801a11e73458790c",
            "placeholder": "​",
            "style": "IPY_MODEL_cbafdb29e09546e39e1223b7a6377478",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "7e5f14d2355f4246bb920a36e4a6bc18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6e32b097e344cab809cd26e640d5df4",
            "max": 270341459,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1be570c4ee544590a5bccc199bda3b1f",
            "value": 270341459
          }
        },
        "97b8438f0ef3476088dbe500c2d3481a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19cd223612584c328f0c0f6773cd23cb",
            "placeholder": "​",
            "style": "IPY_MODEL_6f041f3f37394981ab1b94fe391df6d4",
            "value": " 270M/270M [00:08&lt;00:00, 32.6MB/s]"
          }
        },
        "56c4cd598af04da3aeffb2987b51153e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c16d6f445da04b04801a11e73458790c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cbafdb29e09546e39e1223b7a6377478": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e6e32b097e344cab809cd26e640d5df4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1be570c4ee544590a5bccc199bda3b1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "19cd223612584c328f0c0f6773cd23cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f041f3f37394981ab1b94fe391df6d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Extracting Nouns from the text file using Super Sense Tagging**"
      ],
      "metadata": {
        "id": "uN-_TGnQOORe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install booknlp\n",
        "!python -m spacy download en_core_web_lg"
      ],
      "metadata": {
        "id": "TSLT9cXUQks5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from booknlp.booknlp import BookNLP\n",
        "\n",
        "# model_params={\n",
        "# \t\t\"pipeline\":\"entity,quote,supersense,event,coref\",\n",
        "# \t\t\"model\":\"big\"\n",
        "# \t}\n",
        "\n",
        "######## i am only interested in SuperSenses model parameters so i am using only SuperSense.\n",
        "model_params={\n",
        "\t\t\"pipeline\":\"entity,supersense\",\n",
        "\t\t\"model\":\"big\"\n",
        "\t}\n",
        "\n",
        "\n",
        "booknlp=BookNLP(\"en\", model_params)\n"
      ],
      "metadata": {
        "id": "4cisIKP1Q3wd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304,
          "referenced_widgets": [
            "5e3285b57425419fbad3c3c711286936",
            "828a9d20e65f4e0795b162428868a046",
            "123fee40798f4221a7588c4f04408615",
            "954ccd7a9e244a6dbf748e918619c98f",
            "a0b5cd94fb2748af953123e64667afee",
            "6fea85f894a34d3580671f3edf6e95eb",
            "c3c16bded96a4b5f8a27908c4ccfc77b",
            "12d8043f00234b7f96618fe9a2ec064c",
            "88e8297e8cba4c0a9d4dc22592dc312a",
            "f16914f6724b45c3b51308f94a739275",
            "f8abcb58a8a84417afc8fdb0323a2241",
            "51329aaa77cf44f49ab2eddc36490eb3",
            "934630af56cf45478fd6c05880f80d2e",
            "b051da4e9a314dfe860594468f51f3d3",
            "6a2a68352e2047a6a4c7b3faf6c9a996",
            "0c61ab1c203e4dd287bdd3ca29bd7edc",
            "cf14b71f17f54b87afcd0ead9979e70a",
            "7b2769403d8242c5b78eee617e317b5d",
            "4fa7912822c0449386836f9730a6d07b",
            "5c5cbe4078bf41229aa0b07c12e90c03",
            "223943bf86c14f9dae7f8f3b5381d3bd",
            "f33e68d382834bcf913d89811bd04a00",
            "279791ca2aa44967890eba86b1fb5a99",
            "3c18d16c634f486693b158dd41af3240",
            "7e5f14d2355f4246bb920a36e4a6bc18",
            "97b8438f0ef3476088dbe500c2d3481a",
            "56c4cd598af04da3aeffb2987b51153e",
            "c16d6f445da04b04801a11e73458790c",
            "cbafdb29e09546e39e1223b7a6377478",
            "e6e32b097e344cab809cd26e640d5df4",
            "1be570c4ee544590a5bccc199bda3b1f",
            "19cd223612584c328f0c0f6773cd23cb",
            "6f041f3f37394981ab1b94fe391df6d4"
          ]
        },
        "outputId": "5c2eb477-7765-4c6b-8dac-9a2c554ce4d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using device cpu\n",
            "{'pipeline': 'entity,supersense', 'model': 'big'}\n",
            "downloading entities_google_bert_uncased_L-6_H-768_A-12-v1.0.model\n",
            "downloading coref_google_bert_uncased_L-12_H-768_A-12-v1.0.model\n",
            "downloading speaker_google_bert_uncased_L-12_H-768_A-12-v1.0.1.model\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5e3285b57425419fbad3c3c711286936"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/384 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "51329aaa77cf44f49ab2eddc36490eb3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/270M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "279791ca2aa44967890eba86b1fb5a99"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- startup: 43.512 seconds ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Input file to process\n",
        "input_file=\"train.txt\"\n",
        "\n",
        "# Output directory to store resulting files in\n",
        "output_directory=\"output_dir/bartleby/\"\n",
        "\n",
        "# File within this directory will be named ${book_id}.entities, ${book_id}.tokens, etc.\n",
        "book_id=\"bartleby\"\n",
        "\n",
        "booknlp.process(input_file, output_directory, book_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxE0phRgRQwj",
        "outputId": "58ae791f-4005-4eac-eb28-1984129ec558"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- spacy: 15.908 seconds ---\n",
            "--- entities: 284.568 seconds ---\n",
            "--- quotes: 0.125 seconds ---\n",
            "--- name coref: 4.524 seconds ---\n",
            "--- TOTAL (excl. startup): 310.733 seconds ---, 62348 words\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now open output_dir/bartleby/bartleby.supersense file and copy all content.**\n",
        "1. Open an excel file and paste all copied content to that excel file.\n",
        "2. Perform pre-processing on data."
      ],
      "metadata": {
        "id": "XUrXDaUNRDKp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre-processing includes\n",
        "1. Splitting Nouns only from all SST based senses.\n",
        "2. Catagorical division of data for each sense type.\n",
        "3. Removing in-column duplication from each sense type.\n",
        "4. filtering-out multiple sense problem. i.e, noun present in multiple SST based senses."
      ],
      "metadata": {
        "id": "Ae1WP4OwSbEb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**removing duplicate values in cells.**"
      ],
      "metadata": {
        "id": "xgc0henI2kNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Read in the XLSX file as a dataframe\n",
        "df = pd.read_excel('test.xlsx')\n",
        "\n",
        "# Convert all values to strings\n",
        "df = df.astype(str)\n",
        "\n",
        "# Flatten the dataframe into a list of values\n",
        "values = df.values.flatten()\n",
        "\n",
        "# Convert the list of values to a NumPy array\n",
        "arr = np.array(values)\n",
        "\n",
        "# Find the indices of the duplicate values\n",
        "_, indices = np.unique(arr, return_inverse=True)\n",
        "duplicates = np.where(np.bincount(indices) > 1)[0]\n",
        "\n",
        "# Replace the duplicate values with empty cells\n",
        "for index in duplicates:\n",
        "    values[index] = \"\"\n",
        "\n",
        "# Reshape the list of values back into the original dataframe shape\n",
        "new_df = pd.DataFrame(values.reshape(df.shape), columns=df.columns)\n",
        "\n",
        "# Write the modified dataframe back to the XLSX file\n",
        "new_df.to_excel('modified.xlsx', index=False)\n"
      ],
      "metadata": {
        "id": "DkunwdDo2oks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**converting all cell values to lower-case.**"
      ],
      "metadata": {
        "id": "281OS9Uq3fqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openpyxl\n",
        "\n",
        "# Load the Excel file\n",
        "workbook = openpyxl.load_workbook('modified.xlsx')\n",
        "\n",
        "# Get the active worksheet\n",
        "worksheet = workbook.active\n",
        "\n",
        "# Loop through all cells in the worksheet and convert their values to lower case\n",
        "for row in worksheet.iter_rows():\n",
        "    for cell in row:\n",
        "        cell.value = str(cell.value).lower()\n",
        "\n",
        "# Save the modified Excel file\n",
        "workbook.save('modified1.xlsx')\n"
      ],
      "metadata": {
        "id": "qGjEuGPN3j2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "removing multi-words cells."
      ],
      "metadata": {
        "id": "YV-kAOjL4Zrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openpyxl\n",
        "\n",
        "# Load the Excel file\n",
        "workbook = openpyxl.load_workbook('modified.xlsx')\n",
        "\n",
        "# Get the active worksheet\n",
        "worksheet = workbook.active\n",
        "\n",
        "# Loop through all cells in the worksheet\n",
        "for row in worksheet.iter_rows():\n",
        "    for cell in row:\n",
        "        # Check if the cell value contains multiple words\n",
        "        if isinstance(cell.value, str) and len(cell.value.split()) > 1:\n",
        "            # If the cell value contains multiple words, replace it with an empty cell\n",
        "            cell.value = ''\n",
        "\n",
        "# Save the modified Excel file\n",
        "workbook.save('modified2.xlsx')\n"
      ],
      "metadata": {
        "id": "l3CQYEIN4cjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "replacing all 'nan' with '' empty cells in data."
      ],
      "metadata": {
        "id": "luufx68N8Ksh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openpyxl\n",
        "\n",
        "# Load the Excel file\n",
        "workbook = openpyxl.load_workbook('modified2.xlsx')\n",
        "\n",
        "# Get the active worksheet\n",
        "worksheet = workbook.active\n",
        "\n",
        "# Loop through all cells in the worksheet\n",
        "for row in worksheet.iter_rows():\n",
        "    for cell in row:\n",
        "        # Check if the cell value is None or 'none' (case-insensitive)\n",
        "        if cell.value is None or str(cell.value).lower() == 'none':\n",
        "            # If the cell value is None or 'none', replace it with an empty cell\n",
        "            cell.value = ''\n",
        "\n",
        "# Save the modified Excel file\n",
        "workbook.save('modified3.xlsx')\n"
      ],
      "metadata": {
        "id": "sHt-U2oc73wm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**removing empty cells inside a column.** (if any)"
      ],
      "metadata": {
        "id": "nXiFBjg4rNDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openpyxl\n",
        "\n",
        "# Load the Excel file\n",
        "workbook = openpyxl.load_workbook('modified3.xlsx')\n",
        "\n",
        "# Select the sheet that you want to read and modify\n",
        "worksheet = workbook['Sheet1'] #Replace Sheet1 with your sheet name\n",
        "\n",
        "# Iterate over the columns in the sheet and create a list of non-empty cell values for each column\n",
        "columns = []\n",
        "for column in worksheet.iter_cols():\n",
        "    column_data = []\n",
        "    for cell in column:\n",
        "        if cell.value is not None:\n",
        "            column_data.append(cell.value)\n",
        "    columns.append(column_data)\n",
        "\n",
        "# Iterate over the columns again and write the non-empty cell values back to the sheet\n",
        "for i, column in enumerate(worksheet.columns):\n",
        "    for j, cell in enumerate(column):\n",
        "        if j < len(columns[i]):\n",
        "            cell.value = columns[i][j]\n",
        "        else:\n",
        "            cell.value = None\n",
        "\n",
        "# Save the modified Excel file\n",
        "workbook.save('modified4.xlsx')\n"
      ],
      "metadata": {
        "id": "xGUHzchQrSHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv modified4.xlsx nouns.xlsx"
      ],
      "metadata": {
        "id": "q4S0GPEO6UfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm modified*"
      ],
      "metadata": {
        "id": "kgNPHjzw8eMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The code below performs the following operations**\n",
        "1. reads excel file containing catagorical SST based nouns.\n",
        "2. reads text file containing sentences for dataset.\n",
        "3. pass each sentence through spaCy and extract nouns.\n",
        "4. finds that noun in the excel file.\n",
        "5. replace that noun with another Randomly selected noun by keeping the SST category.\n",
        "6. replace the modified noun in the original sentence.\n",
        "7. writes the modified sentences to a new file.\n",
        "8. writes the dictionary of actual and replaced noun as well to update DRS file."
      ],
      "metadata": {
        "id": "YptQ-zMgTIw8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code works well on small dataset portions. Splitting dataset into files having 400 examples each."
      ],
      "metadata": {
        "id": "D06aY8c-Jpu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "\n",
        "# Set the input file paths\n",
        "sentences_file = 'train.txt.raw'\n",
        "drs_file = 'train.txt'\n",
        "\n",
        "# Set the output file prefix\n",
        "output_prefix = ''\n",
        "\n",
        "# Set the number of sentences per output file\n",
        "sentences_per_file = 400\n",
        "\n",
        "# Create the output directory if it doesn't exist\n",
        "if not os.path.exists('dataset'):\n",
        "    os.mkdir('dataset')\n",
        "\n",
        "# Read in the sentences\n",
        "with open(sentences_file, 'r') as f:\n",
        "    sentences = [line.strip() for line in f.readlines()]\n",
        "\n",
        "# Read in the DRSs\n",
        "with open(drs_file, 'r') as f:\n",
        "    drs = f.read().split('\\n\\n')\n",
        "\n",
        "# Combine the sentences and DRSs into tuples\n",
        "data = list(zip(sentences, drs))\n",
        "\n",
        "# Split the data into chunks of 400 sentences\n",
        "num_chunks = math.ceil(len(data) / sentences_per_file)\n",
        "chunks = [data[i:i+sentences_per_file] for i in range(0, len(data), sentences_per_file)]\n",
        "\n",
        "# Write each chunk to a new file in the output directory\n",
        "for i, chunk in enumerate(chunks):\n",
        "    filename = output_prefix + str(i+1) + '.txt'\n",
        "    filepath = os.path.join('dataset', filename)\n",
        "    with open(filepath, 'w') as f:\n",
        "        for sentence, dr in chunk:\n",
        "            f.write(sentence + '\\n')\n",
        "\n",
        "    dr_filename = output_prefix + str(i+1) + '_drs.txt'\n",
        "    dr_filepath = os.path.join('dataset', dr_filename)\n",
        "    with open(dr_filepath, 'w') as f:\n",
        "        for sentence, dr in chunk:\n",
        "            f.write(dr + '\\n\\n')\n"
      ],
      "metadata": {
        "id": "2M1zvoCKJyvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**working with DRS and Nouns at the same time.**"
      ],
      "metadata": {
        "id": "FjhxhNFhUVIp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "working with large spaCy model."
      ],
      "metadata": {
        "id": "kBAKhAKkVG7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_lg\n"
      ],
      "metadata": {
        "id": "Y1l2nZWFVJ9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import openpyxl\n",
        "import random\n",
        "import re\n",
        "\n",
        "# Load the spacy model\n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "# Load the XLSX file\n",
        "wb = openpyxl.load_workbook(\"nouns.xlsx\")\n",
        "ws = wb.active\n",
        "\n",
        "# Define a dictionary to cache the replacement nouns for each unique noun found in the XLSX file\n",
        "replacement_noun_cache = {}\n",
        "\n",
        "# Define a helper function to find a replacement noun and cache it for future use\n",
        "def get_replacement_noun(noun):\n",
        "    # Check if a replacement noun has already been cached for this noun\n",
        "    if noun in replacement_noun_cache:\n",
        "        return replacement_noun_cache[noun]\n",
        "\n",
        "    # Split the noun into its component parts using regular expressions\n",
        "    noun_parts = re.split(\"[_\\-~]+\", noun)\n",
        "\n",
        "    # Look for a replacement noun in the XLSX file that matches any of the noun parts\n",
        "    for col in ws.iter_cols(values_only=True):\n",
        "        for noun_part in noun_parts:\n",
        "            if noun_part.lower() in [str(cell).lower() for cell in col]:\n",
        "                candidate_nouns = [str(cell) for cell in col if str(cell) != \"None\"]\n",
        "                if candidate_nouns:\n",
        "                    replacement_noun = random.choice(candidate_nouns)\n",
        "                    # Cache the replacement noun for future use\n",
        "                    replacement_noun_cache[noun] = replacement_noun\n",
        "                    return replacement_noun\n",
        "\n",
        "    # If no replacement noun was found, return None\n",
        "    return None\n",
        "\n",
        "# Define the paths to the input files\n",
        "drs_path = \"/content/dataset/17_drs.txt\"\n",
        "text_path = \"/content/dataset/17.txt\"\n",
        "\n",
        "# Define the paths to the output files\n",
        "updated_drs_path = \"/content/noun_aug/17_drs.txt\"\n",
        "updated_text_path = \"/content/noun_aug/17.txt\"\n",
        "\n",
        "# Open the input files\n",
        "with open(drs_path, \"r\") as drs_file, open(text_path, \"r\") as text_file:\n",
        "    # Load the DRS file into a list\n",
        "    drs_lines = drs_file.readlines()\n",
        "\n",
        "    # Open the output files\n",
        "    with open(updated_drs_path, \"w\") as updated_drs_file, open(updated_text_path, \"w\") as updated_text_file:\n",
        "        # Loop over the sentences in the text file\n",
        "        for sentence in text_file:\n",
        "            # Parse the sentence with spacy\n",
        "            doc = nlp(sentence)\n",
        "\n",
        "            # Keep track of the nouns to replace\n",
        "            nouns_to_replace = []\n",
        "\n",
        "            # Find the nouns in the sentence\n",
        "            for token in doc:\n",
        "                if token.pos_ == \"NOUN\":\n",
        "                    noun = token.text.lower()\n",
        "                    if noun not in nouns_to_replace:\n",
        "                        nouns_to_replace.append(noun)\n",
        "\n",
        "            # Replace the nouns in the sentence and the DRS\n",
        "            for noun in nouns_to_replace:\n",
        "                replacement_noun = get_replacement_noun(noun)\n",
        "                if replacement_noun:\n",
        "                    sentence = sentence.replace(noun, replacement_noun)\n",
        "                    drs_lines = [line.replace(noun, replacement_noun) for line in drs_lines]\n",
        "\n",
        "            # Write the updated sentence to the updated text file\n",
        "            updated_text_file.write(sentence)\n",
        "\n",
        "        # Write the updated DRS lines to the updated DRS file\n",
        "        updated_drs_file.writelines(drs_lines)\n"
      ],
      "metadata": {
        "id": "oGjD3PZ6bbb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "combining all splitted dataset files"
      ],
      "metadata": {
        "id": "qHq74HYGSJh8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd noun_aug"
      ],
      "metadata": {
        "id": "ofo6lTcDSS4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "for text files"
      ],
      "metadata": {
        "id": "hOx9zlq2Sf3U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!for i in {1..17}; do cat \"$i.txt\" >> output.txt; done"
      ],
      "metadata": {
        "id": "R_pRPmCfSMKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "for DRS files"
      ],
      "metadata": {
        "id": "Gd8N_gZeShQp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!for i in {1..17}_drs.txt; do cat \"$i\" >> drs_output.txt; done\n"
      ],
      "metadata": {
        "id": "_bQ0E400SiXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "moving augmented files outside the sub-folders"
      ],
      "metadata": {
        "id": "vi3G7aMmTRsT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mv drs_output.txt ..\n",
        "!mv output.txt .."
      ],
      "metadata": {
        "id": "2LeKK5f6TWXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVyW-9xCTin7",
        "outputId": "6c141f37-dac1-4a80-e7df-b1dd09d8aa50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "re-naming augmented files."
      ],
      "metadata": {
        "id": "oAURCd-wUZ2k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mv drs_output.txt noun_aug_drs.txt\n",
        "!mv output.txt noun_aug_text.txt"
      ],
      "metadata": {
        "id": "MNkFBb19T9S0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXtIKFEaV9ZR",
        "outputId": "548e32a3-ae19-4a5b-f531-068a6124a178"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mdataset\u001b[0m/   noun_aug_drs.txt   nouns.xlsx  train.txt.raw\n",
            "\u001b[01;34mnoun_aug\u001b[0m/  noun_aug_text.txt  train.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Adopting an alternative way (dictionary based) to augment nouns.**"
      ],
      "metadata": {
        "id": "oeYozqrH3uzd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "getting nouns in each sentence"
      ],
      "metadata": {
        "id": "HdLe_dC0UJ7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Read the text file\n",
        "with open(\"train.txt.raw\") as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Process the text with spaCy\n",
        "doc = nlp(text)\n",
        "\n",
        "# Iterate through each sentence and print the nouns\n",
        "for sent in doc.sents:\n",
        "    nouns = [token.text for token in sent if token.pos_ == \"NOUN\"]\n",
        "    print(\"Nouns in the sentence '{}': {}\".format(sent.text.strip(), nouns))\n"
      ],
      "metadata": {
        "id": "_j2U__UK3iF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**reading SST based xlsx file to get all nouns in text file.**"
      ],
      "metadata": {
        "id": "o23BmO0N7jbL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the Excel file into a pandas DataFrame\n",
        "df = pd.read_excel(\"train.xlsx\")\n",
        "\n",
        "# Get all non-NaN words in the DataFrame except the first row (which contains the tags), removing any leading or trailing spaces\n",
        "words = df.iloc[1:].values.flatten().tolist()\n",
        "words = [word.strip() for word in words if pd.notna(word)]\n",
        "\n",
        "# Save all the non-NaN words to a single text file\n",
        "with open(\"nouns.txt\", \"w\") as f:\n",
        "    f.write(\"\\n\".join(words))\n"
      ],
      "metadata": {
        "id": "eUWDFoe97o0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "getting nouns in a text file. one noun per line."
      ],
      "metadata": {
        "id": "Pjz_K1kLdvE8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Read the text file\n",
        "with open(\"train.txt.raw\") as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Process the text with spaCy\n",
        "doc = nlp(text)\n",
        "\n",
        "# Open a new file to write the nouns\n",
        "with open(\"nouns.txt\", \"w\") as file:\n",
        "    # Iterate through each sentence and write each noun to a separate line in the file\n",
        "    for sent in doc.sents:\n",
        "        nouns = [token.text for token in sent if token.pos_ == \"NOUN\"]\n",
        "        for noun in nouns:\n",
        "            # Write each noun to a separate line in the file\n",
        "            file.write(noun + \"\\n\")\n"
      ],
      "metadata": {
        "id": "pbg3w8NwaSht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "converting to lower case"
      ],
      "metadata": {
        "id": "Gcgztvrr1Ocf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('nouns.txt', 'r') as file:\n",
        "    text = file.read()\n",
        "    lower_text = text.lower()\n",
        "\n",
        "with open('lower_nouns.txt', 'w') as file:\n",
        "    file.write(lower_text)\n"
      ],
      "metadata": {
        "id": "vWA8KK4q1QUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "getting unique nouns only."
      ],
      "metadata": {
        "id": "_o_GxegbjunA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sort nouns.txt | uniq > unique_nouns.txt"
      ],
      "metadata": {
        "id": "5sDYkIYmjxN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "getting randomly selected nouns from same SST based column. Saving replacable noun to a new file."
      ],
      "metadata": {
        "id": "U1zqY9hQdzpH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Read the xlsx file using pandas\n",
        "df = pd.read_excel('train.xlsx')\n",
        "\n",
        "# Read the text file and store the words in a list\n",
        "with open('unique_nouns.txt', 'r') as f:\n",
        "    words = [line.strip() for line in f]\n",
        "\n",
        "# Create a dictionary to store the updated words\n",
        "updated_words = {}\n",
        "\n",
        "# Iterate over the words from the text file\n",
        "for word in words:\n",
        "    # Iterate over each column in the dataframe\n",
        "    for col in df.columns:\n",
        "        # Check if the word is in the column\n",
        "        if word in df[col].tolist():\n",
        "            # Get a list of non-nan values from the same column\n",
        "            non_nan_values = df[col].dropna().tolist()\n",
        "            # Remove the original word from the list\n",
        "            non_nan_values.remove(word)\n",
        "            # Check if there are any non-nan values left in the list\n",
        "            if non_nan_values:\n",
        "                # Get a random non-nan value from the same column\n",
        "                new_word = random.choice(non_nan_values)\n",
        "                # Store the updated word in the dictionary\n",
        "                updated_words[word] = new_word\n",
        "\n",
        "# Write the updated words to a new text file\n",
        "with open('modified_nouns.txt', 'w') as f:\n",
        "    for word in words:\n",
        "        # Write the original word or the updated word to the file\n",
        "        if word in updated_words:\n",
        "            f.write(updated_words[word] + '\\n')\n",
        "        else:\n",
        "            f.write(word + '\\n')\n"
      ],
      "metadata": {
        "id": "Cst937IDe3eA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**problem with random selection is that, it only updates half of the nouns. therefore, i am taking the next value not the random value.**"
      ],
      "metadata": {
        "id": "s4fsreH9AFSd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the xlsx file using pandas\n",
        "df = pd.read_excel('train.xlsx', header=None)\n",
        "\n",
        "# Read the text file and store the words in a list\n",
        "with open('unique_nouns.txt', 'r') as f:\n",
        "    words = [line.strip() for line in f]\n",
        "\n",
        "# Create a dictionary to store the updated words\n",
        "updated_words = {}\n",
        "\n",
        "# Iterate over the words from the text file\n",
        "for word in words:\n",
        "    # Iterate over each column in the dataframe\n",
        "    for col in df.columns:\n",
        "        # Check if the word is in the column\n",
        "        if word in df[col].tolist():\n",
        "            # Get a list of non-nan values from the same column\n",
        "            non_nan_values = df[col].dropna().tolist()\n",
        "            # Check if the first row in the column contains column names\n",
        "            if df.iloc[0, col] in df.columns:\n",
        "                # If the first row contains column names, skip it\n",
        "                non_nan_values = non_nan_values[1:]\n",
        "            # Check if there is a non-null value after the original word in the list\n",
        "            if word in non_nan_values:\n",
        "                index = non_nan_values.index(word)\n",
        "                if index < len(non_nan_values) - 1:\n",
        "                    # Get the next non-null value from the same column\n",
        "                    new_word = non_nan_values[index + 1]\n",
        "                    # Store the updated word in the dictionary\n",
        "                    updated_words[word] = new_word\n",
        "                elif index == len(non_nan_values) - 1:\n",
        "                    # If the original word is the last non-null value in the column, replace it with the first non-null value\n",
        "                    new_word = non_nan_values[0]\n",
        "                    # Store the updated word in the dictionary\n",
        "                    updated_words[word] = new_word\n",
        "\n",
        "# Write the updated words to a new text file\n",
        "with open('modified_nouns.txt', 'w') as f:\n",
        "    for word in words:\n",
        "        # Write the original word or the updated word to the file\n",
        "        if word in updated_words:\n",
        "            f.write(updated_words[word] + '\\n')\n",
        "        else:\n",
        "            f.write(word + '\\n')\n"
      ],
      "metadata": {
        "id": "rouW43b3ATCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "calculating changed nouns"
      ],
      "metadata": {
        "id": "tMemi9-_4vu-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Open and read the first file\n",
        "with open('unique_nouns.txt', 'r') as f1:\n",
        "    lines1 = f1.readlines()\n",
        "\n",
        "# Open and read the second file\n",
        "with open('modified_nouns.txt', 'r') as f2:\n",
        "    lines2 = f2.readlines()\n",
        "\n",
        "# Convert each line to a list of words\n",
        "words1 = [line.strip() for line in lines1]\n",
        "words2 = [line.strip() for line in lines2]\n",
        "\n",
        "# Find the lines with different words\n",
        "diff_lines = [i for i in range(len(words1)) if words1[i] != words2[i]]\n",
        "\n",
        "# Print the number of lines with different words\n",
        "print(f\"There are {len(diff_lines)} lines with different words.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PQO6lcz4yxa",
        "outputId": "2d002e77-9a10-49bb-b363-d9c9e9649a02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1000 lines with different words.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "generating dictionary of nouns and replaceable nouns."
      ],
      "metadata": {
        "id": "cSOz3Kd1lTAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nouns_dict = {}\n",
        "\n",
        "with open('nouns.txt', 'r') as f1, open('modified_nouns.txt', 'r') as f2:\n",
        "    for line1, line2 in zip(f1, f2):\n",
        "        word1 = line1.strip()\n",
        "        word2 = line2.strip()\n",
        "        nouns_dict[word1] = word2\n",
        "\n",
        "with open('nouns_dict.txt', 'w') as f:\n",
        "    for key, value in nouns_dict.items():\n",
        "        f.write(f\"'{key}' : '{value}',\\n\")\n"
      ],
      "metadata": {
        "id": "Stsep0p8h1K2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PO5tjnKdzpOX",
        "outputId": "1c83f6a4-68b9-4df5-c641-5f2981d98246"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lower_nouns.txt     nouns_dict.txt  train.txt.raw  unique_nouns.txt\n",
            "modified_nouns.txt  nouns.txt       train.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**getting nouns with same sense but not belonging to the same data. i.e., outside dataset but with same sense.**"
      ],
      "metadata": {
        "id": "dS-kjcB2Ow4J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "reading excel data in text file"
      ],
      "metadata": {
        "id": "9rVH3613RiFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load Excel file\n",
        "excel_file_path = 'train.xlsx'\n",
        "df = pd.read_excel(excel_file_path)\n",
        "\n",
        "# Open text file for writing\n",
        "text_file_path = 'train.txt'\n",
        "with open(text_file_path, 'w') as f:\n",
        "    # Iterate through each row in the DataFrame\n",
        "    for _, row in df.iterrows():\n",
        "        # Iterate through each cell in the row\n",
        "        for cell in row:\n",
        "            # Check if cell value is not null\n",
        "            if pd.notna(cell):\n",
        "                # Convert cell value to string, remove leading/trailing spaces, and write to text file\n",
        "                word = str(cell).strip()\n",
        "                f.write(word + '\\n')\n"
      ],
      "metadata": {
        "id": "BUWuYZ6oSgzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFqNvhW_PMe0",
        "outputId": "f9773c52-b86b-4b0c-f3cf-5b9f4a502ada"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "code to use xlsx file for replacement and dictionary generation."
      ],
      "metadata": {
        "id": "oMDZzB4gT5P5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "import pandas as pd\n",
        "\n",
        "# Read Excel file\n",
        "df = pd.read_excel('train.xlsx')\n",
        "\n",
        "# Create an empty dictionary to store actual words and their replacements\n",
        "word_dict = {}\n",
        "\n",
        "# Loop through each column in the DataFrame\n",
        "for col in df.columns:\n",
        "    # Loop through each cell in the column\n",
        "    for i, cell in enumerate(df[col]):\n",
        "        # Check if cell contains a string\n",
        "        if isinstance(cell, str):\n",
        "            # Loop through each word in the cell\n",
        "            for word in cell.split():\n",
        "                # Get synsets for the word\n",
        "                synsets = wn.synsets(word)\n",
        "\n",
        "                # If synsets are found, replace the word with the first lemma of the first synset\n",
        "                if synsets:\n",
        "                    new_word = synsets[0].lemmas()[0].name()\n",
        "                    # Update the dictionary with actual word and replaced word\n",
        "                    word_dict[word] = new_word\n",
        "                    # Replace the word in the cell with the new word\n",
        "                    df.at[i, col] = df.at[i, col].replace(word, new_word)\n",
        "\n",
        "# Save updated Excel file\n",
        "df.to_excel('output.xlsx', index=False)\n",
        "\n",
        "# Save word dictionary to a text file\n",
        "with open('word_dict.txt', 'w') as f:\n",
        "    for key, value in word_dict.items():\n",
        "        f.write(f\"{key}: {value}\\n\")\n"
      ],
      "metadata": {
        "id": "CIwn4kGKO8md"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "code to read processed text file for noun replacement and dictionary generation."
      ],
      "metadata": {
        "id": "PXBLHMkdT-JL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "\n",
        "# Load WordNet\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Read words from the text file\n",
        "with open('train.txt', 'r') as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "# Create a dictionary to store original words and their replaced words\n",
        "word_dict = {}\n",
        "\n",
        "# Replace words with their non-identical synonyms\n",
        "for line in lines:\n",
        "    original_word = line.strip()\n",
        "    synsets = wordnet.synsets(original_word)\n",
        "    if synsets:\n",
        "        synset = synsets[0]  # Get the first synset\n",
        "        lemmas = synset.lemmas()  # Get all lemmas of the synset\n",
        "        for lemma in lemmas:\n",
        "            similar_word = lemma.name()  # Get the lemma's name\n",
        "            if similar_word != original_word:  # Exclude the original word itself\n",
        "                word_dict[original_word] = similar_word\n",
        "                break  # Stop after finding the first non-identical synonym\n",
        "    else:\n",
        "        word_dict[original_word] = original_word\n",
        "\n",
        "# Write the dictionary to a text file\n",
        "with open('word_dict.txt', 'w') as file:\n",
        "    for original_word, similar_word in word_dict.items():\n",
        "        file.write(f\"{original_word}: {similar_word}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vZQpKCrUDXq",
        "outputId": "3ea5223e-f1d1-4e90-e760-f962e6260f2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. using word sense disambiguation for contextual sense based replacement. Getting 1500 contextual synonyms.**"
      ],
      "metadata": {
        "id": "K5rkiLK8V4dR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vm1hhMwCWCCe",
        "outputId": "b3b82a6c-cd76-4dd3-f87b-436619db6d3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "from nltk.wsd import lesk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Load WordNet\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Read words from the text file\n",
        "with open('train.txt', 'r') as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "# Create a dictionary to store original words and their replaced words\n",
        "word_dict = {}\n",
        "\n",
        "# Replace words with their sense-disambiguated synonyms\n",
        "for line in lines:\n",
        "    original_word = line.strip()\n",
        "    synsets = wordnet.synsets(original_word)\n",
        "    if synsets:\n",
        "        sentence = original_word  # Use the original word as the sentence\n",
        "        tokens = word_tokenize(sentence)  # Tokenize the sentence into words\n",
        "        for i, token in enumerate(tokens):\n",
        "            if token == original_word:\n",
        "                synset = lesk(tokens, token)  # Get the Lesk synset for the token\n",
        "                if synset:\n",
        "                    lemmas = synset.lemmas()  # Get all lemmas of the synset\n",
        "                    # Sort lemmas by their frequency count in WordNet\n",
        "                    lemmas = sorted(lemmas, key=lambda x: x.count(), reverse=True)\n",
        "                    for lemma in lemmas:\n",
        "                        similar_word = lemma.name()  # Get the lemma's name\n",
        "                        if similar_word != original_word:\n",
        "                            tokens[i] = similar_word  # Replace the token with the similar word\n",
        "                            word_dict[original_word] = similar_word\n",
        "                            break  # Stop after finding the first non-identical synonym\n",
        "        # Reconstruct the sentence with the replaced words\n",
        "        replaced_sentence = ' '.join(tokens)\n",
        "        #print(f\"Original sentence: {sentence}\")\n",
        "        #print(f\"Replaced sentence: {replaced_sentence}\")\n",
        "    else:\n",
        "        word_dict[original_word] = original_word\n",
        "\n",
        "# Write the dictionary to a text file\n",
        "with open('word_dict.txt', 'w') as file:\n",
        "    for original_word, similar_word in word_dict.items():\n",
        "        file.write(f\"'{original_word}': '{similar_word}',\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxRyDOWGV8BS",
        "outputId": "849983b4-6bd5-4303-aaf7-d94aff3ef4a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**code below will do Noun Augmentation outside data without SS**"
      ],
      "metadata": {
        "id": "iiLUVwynmtzT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "code below will split dictionary based data into words and their corresponding replacement."
      ],
      "metadata": {
        "id": "X8O7tc9NnC94"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Open the text file for reading\n",
        "with open('word_dict.txt', 'r') as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "words = []\n",
        "replacements = []\n",
        "\n",
        "# Split the lines and extract words and replacements\n",
        "for line in lines:\n",
        "    word, replacement = line.strip().split(':')\n",
        "    words.append(word.strip(\"'\"))\n",
        "    replacements.append(replacement.strip(\"',\\n\"))\n",
        "\n",
        "# Save words into a separate file\n",
        "with open('words.txt', 'w') as file:\n",
        "    file.write('\\n'.join(words))\n",
        "\n",
        "# Save replacements into a separate file\n",
        "with open('replacements.txt', 'w') as file:\n",
        "    file.write('\\n'.join(replacements))\n"
      ],
      "metadata": {
        "id": "ukSv46bbm2vi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "code below will replace words with similar new word using wordNet."
      ],
      "metadata": {
        "id": "91syP9a-pmo_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# Initialize NLTK's WordNet\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Open the replacements.txt file\n",
        "with open('replacements.txt', 'r') as file:\n",
        "    replacements = file.readlines()\n",
        "\n",
        "similar_words = []\n",
        "\n",
        "# Iterate over each replacement word\n",
        "for word in replacements:\n",
        "    word = word.strip()\n",
        "\n",
        "    # Get the synsets for the word\n",
        "    synsets = wordnet.synsets(word)\n",
        "\n",
        "    # If there are synsets available, replace the word with the first similar word\n",
        "    if synsets:\n",
        "        similar_word = synsets[0].lemmas()[0].name()\n",
        "        similar_words.append(similar_word)\n",
        "    else:\n",
        "        similar_words.append(word)  # If no synsets found, keep the original word\n",
        "\n",
        "# Save similar words into a separate file\n",
        "with open('similar_words.txt', 'w') as file:\n",
        "    file.write('\\n'.join(similar_words))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBDvDK-mqMW5",
        "outputId": "db2c04c2-eb01-4a23-f366-4c8132705336"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "saving the new dictionary"
      ],
      "metadata": {
        "id": "N22_PzMPtcy4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read words from words.txt\n",
        "with open('words.txt', 'r') as file:\n",
        "    words = file.readlines()\n",
        "\n",
        "# Read similar words from similar_words.txt\n",
        "with open('similar_words.txt', 'r') as file:\n",
        "    similar_words = file.readlines()\n",
        "\n",
        "# Remove newline characters from each word\n",
        "words = [word.strip() for word in words]\n",
        "similar_words = [word.strip() for word in similar_words]\n",
        "\n",
        "# Combine words and similar words into a dictionary\n",
        "dictionary = dict(zip(words, similar_words))\n",
        "\n",
        "# Save dictionary into dic.txt\n",
        "with open('dic.txt', 'w') as file:\n",
        "    for word, similar_word in dictionary.items():\n",
        "        file.write(f\"'{word}':'{similar_word}',\\n\")\n"
      ],
      "metadata": {
        "id": "-9N3jDKEtfP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Noun Augmentation on PMB-5.0.0**"
      ],
      "metadata": {
        "id": "5DAhjY7ohMVx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Applying WSD algorithm to get similar nouns**"
      ],
      "metadata": {
        "id": "Jyuwd5iQhE8c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hRjTurXhb6l",
        "outputId": "ea312619-7115-4d75-d89c-2eaca6097ed5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "from nltk.wsd import lesk  # Word Sense Disambiguation\n",
        "import re\n",
        "\n",
        "# Function to find contextually similar nouns\n",
        "def find_similar_noun(noun):\n",
        "    # Perform Word Sense Disambiguation (WSD) to find the most likely sense\n",
        "    sense = lesk(noun.split(), noun)\n",
        "\n",
        "    if sense:\n",
        "        # Find synonyms based on the chosen sense\n",
        "        synonyms = []\n",
        "        for lemma in sense.lemmas():\n",
        "            synonyms.append(lemma.name())\n",
        "\n",
        "        # Filter out the original noun from the synonyms\n",
        "        synonyms = [synonym for synonym in synonyms if synonym != noun]\n",
        "\n",
        "        # Return the first synonym (you can choose how to handle multiple synonyms)\n",
        "        return synonyms[0] if synonyms else None\n",
        "\n",
        "    return None\n",
        "\n",
        "# Read unique nouns from the \"unique_nouns.txt\" file\n",
        "with open(\"nouns.txt\", \"r\") as file:\n",
        "    unique_nouns = [line.strip() for line in file]\n",
        "\n",
        "# Initialize a list to store replaced nouns\n",
        "replaced_nouns = []\n",
        "\n",
        "# Apply WSD logic to extract similar words for each unique noun\n",
        "for noun in unique_nouns:\n",
        "    similar_word = find_similar_noun(noun)\n",
        "    if similar_word:\n",
        "        replaced_nouns.append(similar_word)\n",
        "    else:\n",
        "        replaced_nouns.append(noun)\n",
        "\n",
        "# Save the replaced nouns to a new text file\n",
        "output_file = \"replaced_nouns.txt\"\n",
        "with open(output_file, \"w\") as file:\n",
        "    for noun in replaced_nouns:\n",
        "        file.write(noun + \"\\n\")\n",
        "\n",
        "print(f\"Replaced nouns saved to {output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybQ7YjsuhLWA",
        "outputId": "67f071c9-f997-4e71-a25c-209e29d1c1c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Replaced nouns saved to replaced_nouns.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "from nltk.wsd import lesk  # Word Sense Disambiguation\n",
        "from nltk.corpus import wordnet_ic  # Information Content\n",
        "import re\n",
        "\n",
        "# Function to find contextually similar nouns\n",
        "def find_similar_noun(noun):\n",
        "    # Perform Word Sense Disambiguation (WSD) to find the most likely sense\n",
        "    sense = lesk(noun.split(), noun)\n",
        "\n",
        "    if sense:\n",
        "        # Find synonyms based on the chosen sense\n",
        "        synonyms = []\n",
        "        for lemma in sense.lemmas():\n",
        "            synonyms.append(lemma.name())\n",
        "\n",
        "        # Filter out the original noun from the synonyms\n",
        "        synonyms = [synonym for synonym in synonyms if synonym != noun]\n",
        "\n",
        "        # Return the first synonym (you can choose how to handle multiple synonyms)\n",
        "        return synonyms[0] if synonyms else None\n",
        "\n",
        "    return None\n",
        "\n",
        "# Function to find synonyms for a noun\n",
        "def find_synonyms(noun):\n",
        "    synonyms = []\n",
        "    for syn in wordnet.synsets(noun):\n",
        "        for lemma in syn.lemmas():\n",
        "            synonyms.append(lemma.name())\n",
        "    return synonyms\n",
        "\n",
        "# Read unique nouns from the \"unique_nouns.txt\" file\n",
        "with open(\"nouns.txt\", \"r\") as file:\n",
        "    unique_nouns = [line.strip() for line in file]\n",
        "\n",
        "# Initialize a list to store replaced nouns\n",
        "replaced_nouns = []\n",
        "\n",
        "# Apply WSD logic to extract similar words for each unique noun\n",
        "for noun in unique_nouns:\n",
        "    similar_word = find_similar_noun(noun)\n",
        "    if similar_word:\n",
        "        replaced_nouns.append(similar_word)\n",
        "    else:\n",
        "        # If WSD didn't change the noun, find synonyms and use the first one\n",
        "        synonyms = find_synonyms(noun)\n",
        "        if synonyms:\n",
        "            replaced_nouns.append(synonyms[0])\n",
        "        else:\n",
        "            # If no synonyms found, keep the original noun\n",
        "            replaced_nouns.append(noun)\n",
        "\n",
        "# Save the replaced nouns to a new text file\n",
        "output_file = \"replaced_nouns2.txt\"\n",
        "with open(output_file, \"w\") as file:\n",
        "    for noun in replaced_nouns:\n",
        "        file.write(noun + \"\\n\")\n",
        "\n",
        "print(f\"Replaced nouns saved to {output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfoSpSBMiRfm",
        "outputId": "b2d28353-231a-4342-8aa1-5cb90b1eedd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Replaced nouns saved to replaced_nouns2.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**extracting plural nouns**"
      ],
      "metadata": {
        "id": "gcCE6uiOVY63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag\n",
        "from nltk.corpus import words\n",
        "\n",
        "# Download the NLTK words dataset if not already downloaded\n",
        "nltk.download('words')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Function to extract plural nouns from a list of words\n",
        "def extract_plural_nouns(word_list):\n",
        "    plural_nouns = []\n",
        "    for word in word_list:\n",
        "        if word.endswith('s') and word in words.words():\n",
        "            plural_nouns.append(word)\n",
        "    return plural_nouns\n",
        "\n",
        "# Input and output file paths\n",
        "input_file_path = 'nouns.txt'\n",
        "output_file_path = 'plural_nouns.txt'\n",
        "\n",
        "# Read the input file containing nouns\n",
        "with open(input_file_path, 'r') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Tokenize the text and perform part-of-speech tagging\n",
        "tokens = word_tokenize(text)\n",
        "tagged_tokens = pos_tag(tokens)\n",
        "\n",
        "# Extract plural nouns\n",
        "plural_nouns = extract_plural_nouns([word for word, tag in tagged_tokens if tag == 'NNS'])\n",
        "\n",
        "# Save the extracted plural nouns to the output file\n",
        "with open(output_file_path, 'w') as f:\n",
        "    for noun in plural_nouns:\n",
        "        f.write(noun + '\\n')\n",
        "\n",
        "print(f\"Extracted {len(plural_nouns)} plural nouns and saved them to '{output_file_path}'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmo-nNv2VbsW",
        "outputId": "292ef8b8-ef23-430b-be87-f5e42378a4d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted 41 plural nouns and saved them to 'plural_nouns.txt'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**extracting multi-word nouns due to their different representation in SBN.**"
      ],
      "metadata": {
        "id": "L97RSckqWbty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the spaCy English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Function to extract multi-word nouns (noun phrases)\n",
        "def extract_multi_word_nouns(text):\n",
        "    doc = nlp(text)\n",
        "    multi_word_nouns = []\n",
        "    for token in doc:\n",
        "        if token.dep_ == 'compound' and token.head.pos_ == 'NOUN':\n",
        "            noun_phrase = ' '.join([token.text for token in token.head.lefts] + [token.head.text] + [token.text for token in token.head.rights])\n",
        "            multi_word_nouns.append(noun_phrase)\n",
        "    return multi_word_nouns\n",
        "\n",
        "# Input and output file paths\n",
        "input_file_path = 'nouns.txt'\n",
        "output_file_path = 'multi_word_nouns.txt'\n",
        "\n",
        "# Read the input file containing text\n",
        "with open(input_file_path, 'r') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Extract multi-word nouns (noun phrases)\n",
        "multi_word_nouns = extract_multi_word_nouns(text)\n",
        "\n",
        "# Save the extracted multi-word nouns to the output file\n",
        "with open(output_file_path, 'w') as f:\n",
        "    for noun_phrase in multi_word_nouns:\n",
        "        f.write(noun_phrase + '\\n')\n",
        "\n",
        "print(f\"Extracted {len(multi_word_nouns)} multi-word nouns and saved them to '{output_file_path}'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4g0z4j6WiiD",
        "outputId": "528fd12b-5b5d-46b8-ae30-b63a45689d6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted 1787 multi-word nouns and saved them to 'multi_word_nouns.txt'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dealing with constraints in noun extraction.**\n",
        "1- singluar plural\n",
        "2- upper case and lower case\n",
        "3- multi-word nouns"
      ],
      "metadata": {
        "id": "zCtP5VJVwl_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install inflect"
      ],
      "metadata": {
        "id": "ik4SdtMVw3kk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import inflect\n",
        "\n",
        "# Initialize the inflect engine for singularization and pluralization\n",
        "p = inflect.engine()\n",
        "\n",
        "# Given data examples\n",
        "data_examples = [\n",
        "    \"It took him three months to learn to ride a bicycle.\ttime.n.08 TPR now male.n.02 quantity.n.01 EQU 3 month.n.02 Quantity -1 learn.v.01 Time -4 Agent -3 Duration -1 Topic +1 ride.v.02 Agent -4 Theme +1 bicycle.n.01\",\n",
        "    \"He is ill.\tmale.n.02 time.n.08 EQU now ill.a.01 AttributeOf -2 Time -1\",\n",
        "    \"They will be jealous.\tperson.n.01 time.n.08 TSU now jealous.a.01 Experiencer -2 Time -1\",\n",
        "    \"I'm running out of ideas.\tperson.n.01 EQU speaker time.n.08 EQU now run_out.v.06 Source -2 Time -1 Theme +1 idea.n.01\",\n",
        "    \"The girl is deeply attached to her aunt.\tgirl.n.01 time.n.08 EQU now deeply.r.01 attached.a.04 Experiencer -3 Time -2 Degree -1 Stimulus +2 female.n.02 person.n.01 Role +1 aunt.n.01 Of -2\",\n",
        "]\n",
        "\n",
        "# Define a function to extract nouns from the logical representation\n",
        "def extract_nouns_from_logical(logical_representation):\n",
        "    nouns = []\n",
        "    # Split the logical representation into components\n",
        "    components = logical_representation.split()\n",
        "    for component in components:\n",
        "        # Extract nouns based on the part-of-speech (\"n\" for nouns) and handle case-insensitivity\n",
        "        match = re.match(r'(.+)\\.n\\.(\\d+)', component, re.IGNORECASE)\n",
        "        if match:\n",
        "            noun = match.group(1)\n",
        "            nouns.append(noun)\n",
        "    return nouns\n",
        "\n",
        "# Define a function to match logical nouns with words in the text\n",
        "def match_logical_nouns_with_text(logical_nouns, text):\n",
        "    matched_nouns = []\n",
        "    # Tokenize the text into words\n",
        "    words = text.split()\n",
        "    for word in words:\n",
        "        # Check if the word corresponds to a logical noun, considering case-insensitivity and singular/plural forms\n",
        "        for noun in logical_nouns:\n",
        "            if (noun.lower() == word.lower() or re.search(rf'\\b{noun}\\b', word, re.IGNORECASE) or\n",
        "                    p.singular_noun(noun) == word.lower() or p.plural_noun(noun) == word.lower()):\n",
        "                matched_nouns.append(word)\n",
        "    return matched_nouns\n",
        "\n",
        "# Extract nouns from each data example and match with words in the text\n",
        "for example in data_examples:\n",
        "    parts = example.split('\\t')\n",
        "    text = parts[0]\n",
        "    logical_representation = parts[1]\n",
        "\n",
        "    # Extract nouns from logical representation\n",
        "    logical_nouns = extract_nouns_from_logical(logical_representation)\n",
        "\n",
        "    # Match logical nouns with words in the text\n",
        "    matched_nouns = match_logical_nouns_with_text(logical_nouns, text)\n",
        "\n",
        "    print(\"Text:\", text)\n",
        "    print(\"Matched Nouns:\", matched_nouns)\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9-IKSmkw2TL",
        "outputId": "77ad5ca4-d34f-4899-eb1d-93ed8d4ad8bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: It took him three months to learn to ride a bicycle.\n",
            "Matched Nouns: ['months', 'bicycle.']\n",
            "\n",
            "Text: He is ill.\n",
            "Matched Nouns: []\n",
            "\n",
            "Text: They will be jealous.\n",
            "Matched Nouns: []\n",
            "\n",
            "Text: I'm running out of ideas.\n",
            "Matched Nouns: []\n",
            "\n",
            "Text: The girl is deeply attached to her aunt.\n",
            "Matched Nouns: ['girl', 'aunt.']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import inflect\n",
        "\n",
        "# Initialize the inflect engine for singularization and pluralization\n",
        "p = inflect.engine()\n",
        "\n",
        "# Define a function to extract nouns from the logical representation\n",
        "def extract_nouns_from_logical(logical_representation):\n",
        "    nouns = []\n",
        "    # Split the logical representation into components\n",
        "    components = logical_representation.split()\n",
        "    for component in components:\n",
        "        # Extract nouns based on the part-of-speech (\"n\" for nouns) and handle case-insensitivity\n",
        "        match = re.match(r'(.+)\\.n\\.(\\d+)', component, re.IGNORECASE)\n",
        "        if match:\n",
        "            noun = match.group(1)\n",
        "            nouns.append(noun)\n",
        "    return nouns\n",
        "\n",
        "# Define a function to match logical nouns with words in the text\n",
        "def match_logical_nouns_with_text(logical_nouns, text):\n",
        "    matched_nouns = []\n",
        "    # Tokenize the text into words\n",
        "    words = text.split()\n",
        "    for word in words:\n",
        "        # Check if the word corresponds to a logical noun, considering case-insensitivity and singular/plural forms\n",
        "        for noun in logical_nouns:\n",
        "            if (noun.lower() == word.lower() or re.search(rf'\\b{noun}\\b', word, re.IGNORECASE) or\n",
        "                    p.singular_noun(noun) == word.lower() or p.plural_noun(noun) == word.lower()):\n",
        "                matched_nouns.append(word)\n",
        "    return matched_nouns\n",
        "\n",
        "# Read data examples from a text file\n",
        "with open(\"train.txt\", \"r\") as file:\n",
        "    data_examples = file.readlines()\n",
        "\n",
        "# Create a file to save the extracted nouns\n",
        "with open(\"extracted_nouns.txt\", \"w\") as noun_file:\n",
        "    for example in data_examples:\n",
        "        example = example.strip()  # Remove leading/trailing whitespace\n",
        "        parts = example.split('\\t')\n",
        "        text = parts[0]\n",
        "        logical_representation = parts[1]\n",
        "\n",
        "        # Extract nouns from logical representation\n",
        "        logical_nouns = extract_nouns_from_logical(logical_representation)\n",
        "\n",
        "        # Match logical nouns with words in the text\n",
        "        matched_nouns = match_logical_nouns_with_text(logical_nouns, text)\n",
        "\n",
        "        # Save the extracted nouns in the noun file with tab separation\n",
        "        noun_file.write(f\"{text}\\t{', '.join(matched_nouns)}\\n\")\n",
        "\n",
        "print(\"Nouns extracted and saved in 'extracted_nouns.txt'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLl9kfO8xcom",
        "outputId": "1dd6db66-b667-4101-aa41-f15f407f9d5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nouns extracted and saved in 'extracted_nouns.txt'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the contents of the input file\n",
        "with open('extracted_nouns.txt', 'r') as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "# Initialize an empty list to store nouns\n",
        "nouns = []\n",
        "\n",
        "# Extract nouns from each line and add them to the list\n",
        "for line in lines:\n",
        "    line = line.strip()\n",
        "    if '\\t' in line:\n",
        "        _, nouns_in_line = line.split('\\t')\n",
        "        if nouns_in_line:\n",
        "            nouns.extend(nouns_in_line.split(', '))\n",
        "\n",
        "# Filter out empty lines and remove duplicate nouns\n",
        "nouns = list(set(filter(None, nouns)))\n",
        "\n",
        "# Write the extracted nouns to the \"nouns.txt\" file\n",
        "with open('nouns.txt', 'w') as output_file:\n",
        "    output_file.write('\\n'.join(nouns))\n",
        "\n",
        "print(\"Nouns extracted and saved to 'nouns.txt'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qP4FH44kKXPZ",
        "outputId": "45c910b4-72ce-45ab-addc-85a2b77719ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nouns extracted and saved to 'nouns.txt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "# Function to remove punctuation marks from a word, except for hyphens\n",
        "def remove_punctuation_except_hyphen(word):\n",
        "    return ''.join(char for char in word if char not in string.punctuation or char == '-')\n",
        "\n",
        "# Read the contents of the \"nouns.txt\" file\n",
        "with open('nouns.txt', 'r') as file:\n",
        "    nouns = file.readlines()\n",
        "\n",
        "# Clean and strip each noun of punctuation marks (except hyphens)\n",
        "cleaned_nouns = [remove_punctuation_except_hyphen(noun.strip()) for noun in nouns]\n",
        "\n",
        "# Write the cleaned nouns to a new file\n",
        "with open('cleaned_nouns.txt', 'w') as output_file:\n",
        "    output_file.write('\\n'.join(cleaned_nouns))\n",
        "\n",
        "print(\"Punctuation removed (except hyphens), and cleaned nouns saved to 'cleaned_nouns.txt'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06Y5SSc-T_QJ",
        "outputId": "676a20c9-ef3f-4c61-b314-582961874d09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Punctuation removed (except hyphens), and cleaned nouns saved to 'cleaned_nouns.txt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sort cleaned_nouns.txt | uniq > unique_nouns.txt"
      ],
      "metadata": {
        "id": "1NHbpbDMNLV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extracting Entities that are the fundamental of SBN notation.**"
      ],
      "metadata": {
        "id": "6gyGaQLYskA1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Regular expression pattern to match generic entities\n",
        "generic_entity_pattern = re.compile(r'^[a-z]+\\.n\\.\\d+$|^[a-z]+\\.a\\.\\d+$|^[a-z]+\\.v\\.\\d+$', re.IGNORECASE)\n",
        "\n",
        "# Function to extract entities excluding generic patterns\n",
        "def extract_entities(sbn):\n",
        "    entities = []\n",
        "    words = sbn.split()\n",
        "    for word in words:\n",
        "        if not generic_entity_pattern.match(word):\n",
        "            entities.append(word)\n",
        "    return entities\n",
        "\n",
        "# Read data from train.txt\n",
        "with open('sbn.txt', 'r') as file:\n",
        "    train_data = file.readlines()\n",
        "\n",
        "# Extract entities and save them to entity.txt, one entity per line\n",
        "with open('entity.txt', 'w') as entity_file:\n",
        "    for sbn in train_data:\n",
        "        entities = extract_entities(sbn)\n",
        "        for entity in entities:\n",
        "            entity_file.write(entity + '\\n')\n",
        "\n",
        "print(\"Entities extracted and saved to entity.txt (one entity per line).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQ_YXAJasvc_",
        "outputId": "a5ee8370-b12a-4488-9c89-18fc69799b0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entities extracted and saved to entity.txt (one entity per line).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sort entity.txt | uniq > unique_entity.txt"
      ],
      "metadata": {
        "id": "brToDj6Qu5Ks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "conveeting to lower case."
      ],
      "metadata": {
        "id": "FarPc-Xm-lcF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('cap_noun_plurals.txt', 'r') as file:\n",
        "    text = file.read()\n",
        "    lower_text = text.lower()\n",
        "\n",
        "with open('lower_nouns_plurals.txt', 'w') as file:\n",
        "    file.write(lower_text)\n"
      ],
      "metadata": {
        "id": "Zg3dbPBu-w8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "converting to cap-case."
      ],
      "metadata": {
        "id": "KTz_0yqp_kmH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Python program to read a file and capitalize\n",
        "# the first letter of every word in the file.\n",
        "\n",
        "# A file named \"gfg\", will be opened with the\n",
        "# reading mode.\n",
        "file_gfg = open('replacable_nouns_lower.txt', 'r')\n",
        "cap_name = open('replacable_nouns_cap.txt','w')\n",
        "\n",
        "# This will traverse through every line one by one\n",
        "# in the file\n",
        "for line in file_gfg:\n",
        "\n",
        "    # This will convert the content\n",
        "    # of that line with capitalized\n",
        "    # first letter of every word\n",
        "    output = line.title()\n",
        "    cap_name.write(output)\n",
        "\n",
        "    # This will print the output\n",
        "    #print(output)"
      ],
      "metadata": {
        "id": "PDDisFtQ_mol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "making plurals of nouns"
      ],
      "metadata": {
        "id": "OUln7RinA2YH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import inflect\n",
        "\n",
        "# Initialize the inflect engine\n",
        "p = inflect.engine()\n",
        "\n",
        "# Read the list of words from a text file\n",
        "# just change the name for lower or upper case file.\n",
        "with open('replacable_nouns_cap.txt', 'r') as file:\n",
        "    words = [line.strip() for line in file]\n",
        "\n",
        "# Generate the plural forms\n",
        "plural_forms = [p.plural(word) for word in words]\n",
        "\n",
        "# Save the plural forms in 'lower_nouns_plurals.txt'\n",
        "with open('rep_cap_nouns_plurals.txt', 'w') as output_file:\n",
        "    for plural in plural_forms:\n",
        "        output_file.write(plural + '\\n')\n",
        "\n",
        "print(\"Plural forms saved in 'lower_nouns_plurals.txt'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xiZCRnbA4Fc",
        "outputId": "f0b316d5-0a52-4cfd-aee2-99a37399d2bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plural forms saved in 'lower_nouns_plurals.txt'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Shuffling Nouns to make inside-context-wo-SS dictionary**"
      ],
      "metadata": {
        "id": "u1iFNIRuj4bH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "\n",
        "# Function to shuffle the content of a text file\n",
        "def shuffle_file_content(input_file, output_file):\n",
        "    # Read data from the input file\n",
        "    with open(input_file, 'r') as file:\n",
        "        data = file.readlines()\n",
        "\n",
        "    # Shuffle the data randomly\n",
        "    random.shuffle(data)\n",
        "\n",
        "    # Write the shuffled data to the output file\n",
        "    with open(output_file, 'w') as file:\n",
        "        file.writelines(data)\n",
        "\n",
        "\n",
        "# Input and output file names\n",
        "input_file_name = 'unique_nouns.txt'  # Replace with your input file name\n",
        "output_file_name = 'rep_unique_nouns.txt'  # Replace with your desired output file name\n",
        "\n",
        "# Shuffle the content of the input file and save it to the output file\n",
        "shuffle_file_content(input_file_name, output_file_name)\n",
        "\n",
        "print(f\"Shuffled content saved to {output_file_name}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGmNp2d6j-fz",
        "outputId": "48b5424c-d28e-4f15-9c29-d7e5d8577540"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shuffled content saved to rep_unique_nouns.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Pre-Processing for Common Noun Augmentation**\n",
        "\n",
        "1.   Converting Extracted nouns from singular + plural --> all singlular.\n",
        "2.   removing duplication. (lower-case-singluar data)\n",
        "3. convert into cap-case. (cap-case-singluar-data)\n",
        "4. converting singluar into plurals for both lower and cap case nouns.\n",
        "\n"
      ],
      "metadata": {
        "id": "6UQdC3d8-GRz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1- converting singluar + plurals into singluar nouns**"
      ],
      "metadata": {
        "id": "kUJz9IC565Xa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install inflect"
      ],
      "metadata": {
        "id": "n4SvOANX7BQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import inflect\n",
        "\n",
        "# Create an inflect engine\n",
        "p = inflect.engine()\n",
        "\n",
        "# Read the list of unique nouns from the file\n",
        "with open(\"nouns.txt\", \"r\") as file:\n",
        "    unique_nouns = [line.strip() for line in file]\n",
        "\n",
        "# Create a list to store the modified nouns\n",
        "modified_nouns = []\n",
        "\n",
        "# Create a dictionary to keep track of the nouns that have been modified\n",
        "modified_dict = {}\n",
        "\n",
        "# Iterate through the unique nouns while preserving order\n",
        "for noun in unique_nouns:\n",
        "    # Check if the noun is plural\n",
        "    if p.singular_noun(noun):\n",
        "        # If it's plural, convert it to its singular form and store it in the dictionary\n",
        "        singular_noun = p.singular_noun(noun)\n",
        "        modified_dict[noun] = singular_noun\n",
        "        modified_nouns.append(singular_noun)\n",
        "    else:\n",
        "        # If it's singular or already converted, keep it as it is\n",
        "        if noun not in modified_dict:\n",
        "            modified_nouns.append(noun)\n",
        "\n",
        "# Write the modified nouns to a new file\n",
        "with open(\"lower_singular_nouns.txt\", \"w\") as output_file:\n",
        "    for noun in modified_nouns:\n",
        "        output_file.write(noun + \"\\n\")\n",
        "\n",
        "# Print a message to indicate that the modified nouns have been saved\n",
        "print(\"Modified nouns have been saved to 'singular_nouns.txt'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wYtMfZ-68_2",
        "outputId": "bbd757a6-940a-4279-caca-d75333c786f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modified nouns have been saved to 'singular_nouns.txt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2- Removing data duplication Nouns without changing the order**"
      ],
      "metadata": {
        "id": "ZbpGBMRc51ik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the list of nouns from a file (one noun per line)\n",
        "with open(\"lower_singular_nouns.txt\", \"r\") as file:\n",
        "    nouns = [line.strip() for line in file]\n",
        "\n",
        "# Create a dictionary to keep track of unique nouns while preserving order\n",
        "unique_nouns_dict = {}\n",
        "\n",
        "# Iterate through the list of nouns\n",
        "unique_nouns = []\n",
        "for noun in nouns:\n",
        "    # Check if the noun is not in the dictionary\n",
        "    if noun not in unique_nouns_dict:\n",
        "        # Add the noun to the dictionary and the list\n",
        "        unique_nouns_dict[noun] = True\n",
        "        unique_nouns.append(noun)\n",
        "\n",
        "# Write the unique nouns to a text file\n",
        "with open(\"unique_lower_singular_nouns.txt\", \"w\") as output_file:\n",
        "    for noun in unique_nouns:\n",
        "        output_file.write(noun + \"\\n\")\n",
        "\n",
        "# Print a message to indicate that the unique nouns have been saved\n",
        "print(\"Unique nouns have been saved to 'unique_nouns.txt'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__HSi1pE55i-",
        "outputId": "420c536c-3a1a-47f6-a0d7-ca91b8a039cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique nouns have been saved to 'unique_nouns.txt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**working with Noun Augmentation based-on WordNet Synset and Hypernym based noun replacements**.\n",
        "\n",
        "*   Dataset is PMB-5.0.0\n",
        "*   Noun Augmentation\n",
        "\n"
      ],
      "metadata": {
        "id": "JAWbROSHM_by"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# Sample list of nouns and the noun to replace\n",
        "noun_list = [\"animal\", \"fruit\", \"vehicle\"]\n",
        "original_noun = \"animal\"\n",
        "\n",
        "# Function to replace a noun with a hyponym\n",
        "def replace_with_hyponym(text, original, hyponym):\n",
        "    return text.replace(original, hyponym)\n",
        "\n",
        "# Find hyponyms for the original noun\n",
        "hyponyms = []\n",
        "for synset in wordnet.synsets(original_noun):\n",
        "    for lemma in synset.lemmas():\n",
        "        hyponyms.append(lemma.name())\n",
        "\n",
        "# Choose a replacement (e.g., randomly)\n",
        "import random\n",
        "replacement = random.choice(hyponyms)\n",
        "\n",
        "# Sample text\n",
        "text = \"The quick brown animal jumps over the lazy dog.\"\n",
        "\n",
        "# Replace the noun in the text\n",
        "new_text = replace_with_hyponym(text, original_noun, replacement)\n",
        "print(new_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImhtZZTGNQaX",
        "outputId": "0422ea9e-30eb-4812-cfb7-8c9b3a4ebddc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The quick brown animal jumps over the lazy dog.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**code for common noun augmentation on dataset file**"
      ],
      "metadata": {
        "id": "mSAuj6MBrUCu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Initialize the WordNet lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Function to check if a word is a common noun and exists in various forms in the text\n",
        "def is_common_noun_in_text(word, text):\n",
        "    # Generate possible forms of the word\n",
        "    forms = set()\n",
        "    forms.add(word.lower())           # Lower-case\n",
        "    forms.add(word.upper())           # Upper-case\n",
        "    forms.add(word.capitalize())      # Cap-case\n",
        "    forms.add(lemmatizer.lemmatize(word.lower()))  # Singular\n",
        "    forms.add(lemmatizer.lemmatize(word.lower(), 'n'))  # Singular (explicitly as noun)\n",
        "    forms.add(lemmatizer.lemmatize(word.lower(), 'v'))  # Base form (verb)\n",
        "    forms.add(word.lower() + 's')     # Plural\n",
        "    forms.add(word.upper() + 'S')     # Plural (upper-case)\n",
        "    forms.add(lemmatizer.lemmatize(word.lower() + 's'))  # Plural (lemmatized)\n",
        "\n",
        "    # Check if any form of the word is present in the text\n",
        "    for form in forms:\n",
        "        if form in text.lower():\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "# Function to replace common nouns with hypernyms, considering case sensitivity and singular/plural\n",
        "def replace_common_nouns_with_hypernyms(sentence, logical_representation):\n",
        "    # Tokenize the sentence\n",
        "    tokens = nltk.word_tokenize(sentence)\n",
        "\n",
        "    # Extract nouns from the logical representation for all possible senses (from .n.01 to .n.09)\n",
        "    logical_nouns = []\n",
        "    for sense in range(1, 10):  # Iterate from .n.01 to .n.09\n",
        "        logical_nouns += [word.split('.')[0] for word in logical_representation.split() if word.endswith(f'.n.{sense:02d}')]\n",
        "\n",
        "    # Replace common nouns with hypernyms\n",
        "    replaced_sentence = sentence\n",
        "\n",
        "    for word in logical_nouns:\n",
        "        if is_common_noun_in_text(word, sentence):\n",
        "            synsets = wordnet.synsets(word)\n",
        "            if synsets:\n",
        "                hypernym = synsets[0].hypernyms()  # Choose the first hypernym for simplicity\n",
        "                if hypernym:\n",
        "                    hypernym_word = hypernym[0].name().split('.')[0]\n",
        "                    # Replace nouns in both case-sensitive and case-insensitive forms\n",
        "                    replaced_sentence = replaced_sentence.replace(word, hypernym_word)\n",
        "                    replaced_sentence = replaced_sentence.replace(word.capitalize(), hypernym_word.capitalize())\n",
        "                    logical_representation = logical_representation.replace(word, hypernym_word)\n",
        "\n",
        "    return replaced_sentence, logical_representation\n",
        "\n",
        "# Read the dataset from \"gold.sbn\"\n",
        "input_filename = \"gold.sbn\"\n",
        "output_filename = \"output.sbn\"\n",
        "\n",
        "with open(input_filename, \"r\") as infile:\n",
        "    lines = infile.readlines()\n",
        "\n",
        "# Perform common noun replacement and save the output\n",
        "replaced_dataset = []\n",
        "\n",
        "for line in lines:\n",
        "    sentence, logical_representation = line.strip().split('\\t')\n",
        "    replaced_sentence, replaced_logical_representation = replace_common_nouns_with_hypernyms(sentence, logical_representation)\n",
        "    replaced_dataset.append((replaced_sentence, replaced_logical_representation))\n",
        "\n",
        "# Save the output to \"output.sbn\"\n",
        "with open(output_filename, \"w\") as outfile:\n",
        "    for sentence, logical_representation in replaced_dataset:\n",
        "        outfile.write(f\"{sentence}\\t{logical_representation}\\n\")\n"
      ],
      "metadata": {
        "id": "0VQzFAGSuD4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import random  # Import the random module\n",
        "\n",
        "# Initialize the WordNet lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Function to check if a word is a common noun and exists in various forms in the text\n",
        "def is_common_noun_in_text(word, text):\n",
        "    # Generate possible forms of the word\n",
        "    forms = set()\n",
        "    forms.add(word.lower())  # Lower-case\n",
        "    forms.add(word.upper())  # Upper-case\n",
        "    forms.add(word.capitalize())  # Cap-case\n",
        "    forms.add(lemmatizer.lemmatize(word.lower()))  # Singular\n",
        "    forms.add(lemmatizer.lemmatize(word.lower(), 'n'))  # Singular (explicitly as noun)\n",
        "    forms.add(lemmatizer.lemmatize(word.lower(), 'v'))  # Base form (verb)\n",
        "    forms.add(word.lower() + 's')  # Plural\n",
        "    forms.add(word.upper() + 'S')  # Plural (upper-case)\n",
        "    forms.add(lemmatizer.lemmatize(word.lower() + 's'))  # Plural (lemmatized)\n",
        "\n",
        "    # Check if any form of the word is present in the text\n",
        "    for form in forms:\n",
        "        if form in text.lower():\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "# Function to replace common nouns with random hypernyms\n",
        "def replace_common_nouns_with_hyponyms(sentence, logical_representation):\n",
        "    # Tokenize the sentence\n",
        "    tokens = nltk.word_tokenize(sentence)\n",
        "\n",
        "    # Extract nouns from the logical representation for all possible senses (from .n.01 to .n.09)\n",
        "    logical_nouns = []\n",
        "    for sense in range(1, 10):  # Iterate from .n.01 to .n.09\n",
        "        logical_nouns += [word.split('.')[0] for word in logical_representation.split() if\n",
        "                          word.endswith(f'.n.{sense:02d}')]\n",
        "\n",
        "    # Replace common nouns with random hypernyms\n",
        "    replaced_sentence = sentence\n",
        "\n",
        "    for word in logical_nouns:\n",
        "        if is_common_noun_in_text(word, sentence):\n",
        "            synsets = wordnet.synsets(word)\n",
        "            if synsets:\n",
        "                hyponyms = []\n",
        "                for synset in synsets:\n",
        "                    hyponyms.extend(synset.hyponyms())  # Get a list of hyponyms for each synset\n",
        "\n",
        "                if hyponyms:\n",
        "                    # Choose a random hyponym from the list\n",
        "                    random_hyponym = random.choice(hyponyms)\n",
        "                    hyponym_word = random_hyponym.name().split('.')[0]\n",
        "                    # Replace nouns in both case-sensitive and case-insensitive forms\n",
        "                    replaced_sentence = replaced_sentence.replace(word, hyponym_word)\n",
        "                    replaced_sentence = replaced_sentence.replace(word.capitalize(), hyponym_word.capitalize())\n",
        "                    logical_representation = logical_representation.replace(word, hyponym_word)\n",
        "\n",
        "    return replaced_sentence, logical_representation\n",
        "\n",
        "    # for word in logical_nouns:\n",
        "    #     if is_common_noun_in_text(word, sentence):\n",
        "    #         synsets = wordnet.synsets(word)\n",
        "    #         if synsets:\n",
        "    #             hypernyms = synsets[0].hypernyms()  # Get a list of hypernyms\n",
        "    #             if hypernyms:\n",
        "    #                 # Choose a random hypernym from the list\n",
        "    #                 random_hypernym = random.choice(hypernyms)\n",
        "    #                 hypernym_word = random_hypernym.name().split('.')[0]\n",
        "    #                 # Replace nouns in both case-sensitive and case-insensitive forms\n",
        "    #                 replaced_sentence = replaced_sentence.replace(word, hypernym_word)\n",
        "    #                 replaced_sentence = replaced_sentence.replace(word.capitalize(), hypernym_word.capitalize())\n",
        "    #                 logical_representation = logical_representation.replace(word, hypernym_word)\n",
        "    #\n",
        "    # return replaced_sentence, logical_representation\n",
        "\n",
        "# Read the dataset from \"gold.sbn\"\n",
        "input_filename = \"gold.sbn\"\n",
        "output_filename = \"noun_replacement_through_random_hyponyms.sbn\"\n",
        "\n",
        "with open(input_filename, \"r\") as infile:\n",
        "    lines = infile.readlines()\n",
        "\n",
        "# Perform common noun replacement and save the output\n",
        "replaced_dataset = []\n",
        "\n",
        "for line in lines:\n",
        "    sentence, logical_representation = line.strip().split('\\t')\n",
        "    replaced_sentence, replaced_logical_representation = replace_common_nouns_with_hyponyms(sentence,\n",
        "                                                                                             logical_representation)\n",
        "    replaced_dataset.append((replaced_sentence, replaced_logical_representation))\n",
        "\n",
        "# Save the output to \"noun_replacement_through_first_hypernym.sbn\"\n",
        "with open(output_filename, \"w\") as outfile:\n",
        "    for sentence, logical_representation in replaced_dataset:\n",
        "        outfile.write(f\"{sentence}\\t{logical_representation}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcH8LrYUN-Dv",
        "outputId": "5693c1d4-9457-4a34-ac34-fa261c73e083"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    }
  ]
}