{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Getting all sentences in text file i.e., train.txt.raw having a specific noun. Noun_list is the list of all nouns having multiple presence in sentences.\n",
        "1. this will read word-list\n",
        "2. find all sentences belonging to each word in word-list\n",
        "3. save each file saperately with the name of word from word list\n",
        "4. currently, we are focusing only Train file of Gold-PMB"
      ],
      "metadata": {
        "id": "waY6wGk9F7Lo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the code below will get all the sentences where noun is used including prefix and postfix. e.g., for arm, it will also give bendarm, and armwrestling etc ..."
      ],
      "metadata": {
        "id": "HNYfdoRSYLSO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEG7J3HKFoWy"
      },
      "outputs": [],
      "source": [
        "# Open and read the file containing the list of words\n",
        "with open('word_list.txt.raw', 'r') as f:\n",
        "    word_list = [line.strip() for line in f.readlines()]\n",
        "\n",
        "# Create a dictionary to store sentences by word\n",
        "sentences_by_word = {word: [] for word in word_list}\n",
        "\n",
        "# Open and read the file containing sentences\n",
        "with open('sentences.txt.raw', 'r') as f:\n",
        "    sentences = f.readlines()\n",
        "\n",
        "# Loop through each sentence and check if it contains any of the words\n",
        "for sentence in sentences:\n",
        "    for word in word_list:\n",
        "        if word in sentence:\n",
        "            sentences_by_word[word].append(sentence.strip())\n",
        "\n",
        "# Print out the sentences for each word and save them to a file\n",
        "for word in word_list:\n",
        "    print(f\"Sentences containing '{word}':\")\n",
        "    with open(f\"/content/{word}_sentences.txt\", \"w\") as f:\n",
        "        for sentence in sentences_by_word[word]:\n",
        "            print(sentence)\n",
        "            f.write(sentence + \"\\n\")\n",
        "    print(f\"Sentences containing '{word}' have been saved to '{word}_sentences.txt'.\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the code below will return only those sentences that have exactly the noun given in the sentence without any prefix or postfix values."
      ],
      "metadata": {
        "id": "oJrkni40Yb2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load word list\n",
        "with open(\"word_list.txt.raw\", \"r\") as f:\n",
        "    word_list = [word.strip() for word in f.readlines()]\n",
        "\n",
        "# Open a file for each word in the word list\n",
        "word_files = {word: open(f\"{word}_sentences.txt\", \"w\") for word in word_list}\n",
        "\n",
        "# Read the text file line by line and write matching sentences to the corresponding word files\n",
        "with open(\"sentences.txt.raw\", \"r\") as f:\n",
        "    for line in f:\n",
        "        sentence_words = line.strip().split()\n",
        "        for word in word_list:\n",
        "            if word in sentence_words:\n",
        "                word_files[word].write(line)\n",
        "\n",
        "# Close all word files\n",
        "for file in word_files.values():\n",
        "    file.close()\n"
      ],
      "metadata": {
        "id": "szRtEIS3V5uF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**code below will move all the text files generated through above code into sentences folder.**"
      ],
      "metadata": {
        "id": "Ui7hu2ecHvrY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Define the name of the directory where we want to move the files\n",
        "directory_name = \"sentences\"\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "if not os.path.exists(directory_name):\n",
        "    os.mkdir(directory_name)\n",
        "\n",
        "# Get a list of all the files in the current directory\n",
        "files = os.listdir()\n",
        "\n",
        "# Loop through the files and move any text files to the 'sentences' directory\n",
        "for file in files:\n",
        "    if file.endswith(\".txt\"):\n",
        "        source_path = os.path.abspath(file)\n",
        "        destination_path = os.path.join(os.getcwd(), directory_name, file)\n",
        "        shutil.move(source_path, destination_path)\n",
        "        print(f\"Moved {file} to {directory_name}\")\n"
      ],
      "metadata": {
        "id": "FK-Mx4m5Hwey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**implementing supersense tagging code to each text file to get the supersense for each noun used in multiple sentences.**"
      ],
      "metadata": {
        "id": "c-mU8uk8H-JZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install booknlp\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "vl8lZcXYIIoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from booknlp.booknlp import BookNLP\n",
        "\n",
        "# model_params={\n",
        "# \t\t\"pipeline\":\"entity,quote,supersense,event,coref\",\n",
        "# \t\t\"model\":\"big\"\n",
        "# \t}\n",
        "\n",
        "######## i am only interested in SuperSenses model parameters so i am using only SuperSense.\n",
        "model_params={\n",
        "\t\t\"pipeline\":\"entity,supersense\",\n",
        "\t\t\"model\":\"big\"\n",
        "\t}\n",
        "\n",
        "\n",
        "booknlp=BookNLP(\"en\", model_params)\n"
      ],
      "metadata": {
        "id": "hgrxktg2INu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**code below will deal with 1 file only.**"
      ],
      "metadata": {
        "id": "TmevBChLQ5ON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Input file to process\n",
        "input_file=\"CD_sentences.txt\"\n",
        "\n",
        "# Output directory to store resulting files in\n",
        "output_directory=\"output_dir/bartleby/\"\n",
        "\n",
        "# File within this directory will be named ${book_id}.entities, ${book_id}.tokens, etc.\n",
        "book_id=\"bartleby\"\n",
        "\n",
        "booknlp.process(input_file, output_directory, book_id)"
      ],
      "metadata": {
        "id": "Di0JuRKFITY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**code to read all files one by one and save the processed data to each saperate folder.**"
      ],
      "metadata": {
        "id": "rDH8A_fPKdog"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "#import booknlp\n",
        "\n",
        "# set the directory where the text files are located\n",
        "input_directory = '/content/sentences/'\n",
        "\n",
        "# loop through all files in the input directory\n",
        "for input_file in os.listdir(input_directory):\n",
        "    # check if the file has the .txt extension\n",
        "    if input_file.endswith('.txt'):\n",
        "        # create an output directory for the file\n",
        "        output_directory = os.path.join('/content/sst_outputs', os.path.splitext(input_file)[0])\n",
        "        os.makedirs(output_directory, exist_ok=True)\n",
        "        # File within this directory will be named ${book_id}.entities, ${book_id}.tokens, etc.\n",
        "        book_id = os.path.splitext(input_file)[0]\n",
        "        # process the input file and save the output to the output directory\n",
        "        booknlp.process(os.path.join(input_directory, input_file), output_directory, book_id)\n"
      ],
      "metadata": {
        "id": "Yo1XilRQKhLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "this code will download the 2 folder"
      ],
      "metadata": {
        "id": "6LwuNOXKRGBT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# compress the two folders into a zip archive\n",
        "shutil.make_archive(\"folders\", \"zip\", \"/content/sst_outputs\", \"/content/sentences\")\n",
        "\n",
        "# download the zip archive to your local machine\n",
        "from google.colab import files\n",
        "files.download(\"folders.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "GivQCu4kQ1UZ",
        "outputId": "42e749ae-7878-4416-93c7-7dcfba2faf27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0a46d188-6b0d-4021-a280-59dcc0202370\", \"folders.zip\", 31112)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**splitting dataset files into sub-filels**"
      ],
      "metadata": {
        "id": "zkh6XWxvnIbA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "splitting text files"
      ],
      "metadata": {
        "id": "p20QgIXrrWGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chunks(l, n):\n",
        "    \"\"\"Chunks iterable into n sized chunks\"\"\"\n",
        "    for i in range(0, len(l), n):\n",
        "        yield l[i:i + n]\n",
        "\n",
        "# Collect all lines, without loading whole file into memory\n",
        "lines = []\n",
        "with open('dev.txt.raw') as main_file:\n",
        "    for line in main_file:\n",
        "        lines.append(line)\n",
        "\n",
        "# Write each group of lines to separate files\n",
        "for i, group in enumerate(chunks(lines, n=1), start=1):\n",
        "    with open('File%d.txt' % i, mode=\"w\") as out_file:\n",
        "        for line in group:\n",
        "            out_file.write(line)"
      ],
      "metadata": {
        "id": "ebeSPgrTnNxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "moving files to folder"
      ],
      "metadata": {
        "id": "62I9f5GEk4_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Define the name of the directory where we want to move the files\n",
        "directory_name = \"text-files\"\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "if not os.path.exists(directory_name):\n",
        "    os.mkdir(directory_name)\n",
        "\n",
        "# Get a list of all the files in the current directory\n",
        "files = os.listdir()\n",
        "\n",
        "# Loop through the files and move any text files to the 'sentences' directory\n",
        "for file in files:\n",
        "    if file.endswith(\".txt\"):\n",
        "        source_path = os.path.abspath(file)\n",
        "        destination_path = os.path.join(os.getcwd(), directory_name, file)\n",
        "        shutil.move(source_path, destination_path)\n",
        "        print(f\"Moved {file} to {directory_name}\")\n"
      ],
      "metadata": {
        "id": "3BdAGJymk7lV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**to varify number of files in each folder. we count files on both folders.**"
      ],
      "metadata": {
        "id": "TmXDu5gAySIN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# folder path\n",
        "dir_path = r'/content/text-files'\n",
        "count = 0\n",
        "# Iterate directory\n",
        "for path in os.listdir(dir_path):\n",
        "    # check if current path is a file\n",
        "    if os.path.isfile(os.path.join(dir_path, path)):\n",
        "        count += 1\n",
        "print('File count:', count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDluuw6_yaGB",
        "outputId": "3f57116b-881c-4232-e57a-e4cbdb0f7fe5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File count: 885\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "splitting DRS files"
      ],
      "metadata": {
        "id": "9OFVw8HdrZQF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_file(name, lines_per_chunk, chunks_per_file):\n",
        "\n",
        "    def write_chunk(chunk_no, chunk):\n",
        "        with open(\"file{}.txt\".format(chunk_no), \"w\") as outfile:\n",
        "            outfile.write(\"\".join(i for i in chunk))\n",
        "\n",
        "    count, chunk_no, chunk_count, chunk = 1, 1, 0, []\n",
        "    with open(name, \"r\") as f:\n",
        "        for row in f:\n",
        "            if count > lines_per_chunk and row == \"\\n\":\n",
        "                chunk_count += 1\n",
        "                count = 1\n",
        "                chunk.append(\"\\n\")\n",
        "                if chunk_count == chunks_per_file:\n",
        "                    write_chunk(chunk_no, chunk)\n",
        "                    chunk = []\n",
        "                    chunk_count = 0\n",
        "                    chunk_no += 1\n",
        "            else:\n",
        "                count += 1\n",
        "                chunk.append(row)\n",
        "    if chunk:\n",
        "        write_chunk(chunk_no, chunk)\n",
        "\n",
        "chunk_file(\"dev.txt\", 3, 1)"
      ],
      "metadata": {
        "id": "pX9rzWEhrcTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**moving all text files into \"folder-name\" folder**"
      ],
      "metadata": {
        "id": "aqagQNNOoIiq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Define the name of the directory where we want to move the files\n",
        "directory_name = \"DRS-files\"\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "if not os.path.exists(directory_name):\n",
        "    os.mkdir(directory_name)\n",
        "\n",
        "# Get a list of all the files in the current directory\n",
        "files = os.listdir()\n",
        "\n",
        "# Loop through the files and move any text files to the 'sentences' directory\n",
        "for file in files:\n",
        "    if file.endswith(\".txt\"):\n",
        "        source_path = os.path.abspath(file)\n",
        "        destination_path = os.path.join(os.getcwd(), directory_name, file)\n",
        "        shutil.move(source_path, destination_path)\n",
        "        print(f\"Moved {file} to {directory_name}\")\n"
      ],
      "metadata": {
        "id": "fK-QxXEsoQGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mv ./DRS-files/dev.txt ."
      ],
      "metadata": {
        "id": "RgCWbugilO0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "counting files in the folder"
      ],
      "metadata": {
        "id": "8XifxhTqlcNP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# folder path\n",
        "dir_path = r'/content/DRS-files'\n",
        "count = 0\n",
        "# Iterate directory\n",
        "for path in os.listdir(dir_path):\n",
        "    # check if current path is a file\n",
        "    if os.path.isfile(os.path.join(dir_path, path)):\n",
        "        count += 1\n",
        "print('File count:', count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwnHhhICleT8",
        "outputId": "81645c86-674b-4634-9e97-63ed60ee0466"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File count: 885\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "reading files one by one"
      ],
      "metadata": {
        "id": "V5uRLpWVu4wq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy"
      ],
      "metadata": {
        "id": "PHRD4ZrYvS1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "EJGqztLXvVVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the required libraries\n",
        "import os\n",
        "\n",
        "# Define the location of the directory\n",
        "path =r\"/content/text-files/\"\n",
        "\n",
        "# Change the directory\n",
        "os.chdir(path)\n",
        "\n",
        "def read_files(file_path):\n",
        "   with open(file_path, 'r') as file:\n",
        "      content = file.read()\n",
        "      document = nlp(content)\n",
        "      for token in document:\n",
        "        if token.pos_ == \"NOUN\":\n",
        "          print(token, token.pos_)\n",
        "      print(content)\n",
        "\n",
        "# Iterate over all the files in the directory\n",
        "for file in os.listdir():\n",
        "   if file.endswith('.txt'):\n",
        "      # Create the filepath of particular file\n",
        "      file_path =f\"{path}/{file}\"\n",
        "\n",
        "read_files(file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3IN8foxwjs5",
        "outputId": "265099b1-63f0-4016-dc56-d4bd39a7870e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "news NOUN\n",
            "That news got around.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import errno\n",
        "\n",
        "path = '/content/text-files/*.txt'\n",
        "files = glob.glob(path)\n",
        "for name in files:\n",
        "    try:\n",
        "        with open(name) as f:\n",
        "          content = open(name)\n",
        "          print(content)\n",
        "            #pass # do what you want\n",
        "    except IOError as exc:\n",
        "        if exc.errno != errno.EISDIR:\n",
        "            raise"
      ],
      "metadata": {
        "id": "W3zPXGbCBafe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reading both DRS and Text files at a time and processing them.**"
      ],
      "metadata": {
        "id": "9m6T8ug_kOnb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy"
      ],
      "metadata": {
        "id": "FwilRo3vkbiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "NvflhJyrlrww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install booknlp"
      ],
      "metadata": {
        "id": "yeMSCAtGuEST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Splitting Text Files**"
      ],
      "metadata": {
        "id": "yc51_qUbxgzR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Create a folder to store the sub-files\n",
        "folder_name = 'subfiles'\n",
        "if not os.path.exists(folder_name):\n",
        "    os.makedirs(folder_name)\n",
        "\n",
        "# Open the input file for reading\n",
        "with open('translation.txt', 'r') as input_file:\n",
        "\n",
        "    # Read the contents of the input file\n",
        "    contents = input_file.read()\n",
        "\n",
        "    # Split the contents based on newlines\n",
        "    subfiles = contents.split('\\n')\n",
        "\n",
        "    # Loop over the subfiles and save them in the folder\n",
        "    for i, subfile in enumerate(subfiles):\n",
        "        if subfile:\n",
        "            # Create the path to the output file\n",
        "            output_file_path = os.path.join(folder_name, f'file_{i}.txt')\n",
        "\n",
        "            # Open a new file for writing\n",
        "            with open(output_file_path, 'w') as output_file:\n",
        "                # Write the contents of the subfile to the output file\n",
        "                output_file.write(subfile)\n"
      ],
      "metadata": {
        "id": "eCWtZKrskV_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**splitting DRS files**"
      ],
      "metadata": {
        "id": "t38a6v6rxmdw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Create a folder to store the sub-files\n",
        "folder_name = 'drs_files'\n",
        "if not os.path.exists(folder_name):\n",
        "    os.makedirs(folder_name)\n",
        "\n",
        "# Open the input file for reading\n",
        "with open('drs.txt', 'r') as input_file:\n",
        "\n",
        "    # Read the contents of the input file\n",
        "    contents = input_file.read()\n",
        "\n",
        "    # Split the contents based on newlines\n",
        "    subfiles = contents.split('\\n\\n')\n",
        "\n",
        "    # Loop over the subfiles and save them in the folder\n",
        "    for i, subfile in enumerate(subfiles):\n",
        "        if subfile:\n",
        "            # Create the path to the output file\n",
        "            output_file_path = os.path.join(folder_name, f'file_{i}.txt')\n",
        "\n",
        "            # Open a new file for writing\n",
        "            with open(output_file_path, 'w') as output_file:\n",
        "                # Write the contents of the subfile to the output file\n",
        "                output_file.write(subfile)\n"
      ],
      "metadata": {
        "id": "3cj3ArLIxobd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**modifying text folder with noun_tags**"
      ],
      "metadata": {
        "id": "kNd334WHzpaU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import spacy\n",
        "\n",
        "# Load the spacy model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Create a folder to store the modified sub-files\n",
        "output_folder_name = 'text_files_modified'\n",
        "if not os.path.exists(output_folder_name):\n",
        "    os.makedirs(output_folder_name)\n",
        "\n",
        "# Loop over the files in the subfiles folder\n",
        "input_folder_name = 'text_files'\n",
        "for input_file_name in os.listdir(input_folder_name):\n",
        "    input_file_path = os.path.join(input_folder_name, input_file_name)\n",
        "\n",
        "    # Open the input file for reading\n",
        "    with open(input_file_path, 'r') as input_file:\n",
        "        # Read the contents of the input file\n",
        "        contents = input_file.read()\n",
        "\n",
        "        # Tokenize the contents using spacy\n",
        "        doc = nlp(contents)\n",
        "\n",
        "        # Replace all nouns with noun_tag\n",
        "        new_tokens = []\n",
        "        for token in doc:\n",
        "            if token.pos_ == 'NOUN':\n",
        "                new_tokens.append(nlp.make_doc('noun_tag')[0])\n",
        "            else:\n",
        "                new_tokens.append(token)\n",
        "\n",
        "        # Create a new doc with the modified tokens\n",
        "        modified_doc = spacy.tokens.Doc(doc.vocab, words=[token.text for token in new_tokens])\n",
        "\n",
        "        # Get the modified contents\n",
        "        modified_contents = modified_doc.text\n",
        "\n",
        "    # Create the path to the output file\n",
        "    output_file_path = os.path.join(output_folder_name, input_file_name)\n",
        "\n",
        "    # Open a new file for writing\n",
        "    with open(output_file_path, 'w') as output_file:\n",
        "        # Write the modified contents to the output file\n",
        "        output_file.write(modified_contents)\n"
      ],
      "metadata": {
        "id": "MFonOQk_zpGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**reading nouns for each text file and saving them in another folder's files.**"
      ],
      "metadata": {
        "id": "V5jBxIIQ5fRu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import spacy\n",
        "\n",
        "# Load the spacy model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Define the input and output folder paths\n",
        "input_folder_path = \"text_files\"\n",
        "output_folder_path = \"noun_files\"\n",
        "\n",
        "# Make the output folder if it doesn't already exist\n",
        "if not os.path.exists(output_folder_path):\n",
        "    os.makedirs(output_folder_path)\n",
        "\n",
        "# Loop over the files in the input folder\n",
        "for filename in os.listdir(input_folder_path):\n",
        "    # Get the full path of the input file\n",
        "    input_file_path = os.path.join(input_folder_path, filename)\n",
        "\n",
        "    # Read the contents of the input file\n",
        "    with open(input_file_path, 'r') as f:\n",
        "        contents = f.read()\n",
        "\n",
        "    # Process the contents of the input file\n",
        "    doc = nlp(contents)\n",
        "    nouns = [token.text for token in doc if token.pos_ == 'NOUN']\n",
        "\n",
        "    # Create the output file path\n",
        "    output_file_path = os.path.join(output_folder_path, f'{filename}')\n",
        "\n",
        "    # Write the nouns to the output file\n",
        "    with open(output_file_path, 'w') as f:\n",
        "        f.write('\\n'.join(nouns))\n"
      ],
      "metadata": {
        "id": "ayn6ZQl25qaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**modifying DRS files**"
      ],
      "metadata": {
        "id": "jG-Ghm_J28Ee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define the paths to the two folders\n",
        "folder1_path = \"noun_files\"\n",
        "folder2_path = \"drs_files\"\n",
        "output_path = \"drs_files_modified\"\n",
        "\n",
        "# Create the output directory if it doesn't exist\n",
        "if not os.path.exists(output_path):\n",
        "    os.mkdir(output_path)\n",
        "\n",
        "# Iterate over all the files in folder 1\n",
        "for file1_name in os.listdir(folder1_path):\n",
        "    # Check if the file is a text file\n",
        "    if file1_name.endswith(\".txt\"):\n",
        "        # Get the full path to the file\n",
        "        file1_path = os.path.join(folder1_path, file1_name)\n",
        "\n",
        "        # Read the contents of file 1\n",
        "        with open(file1_path, 'r') as f:\n",
        "            file1_contents = f.read()\n",
        "\n",
        "        # Get the corresponding file in folder 2\n",
        "        file2_name = file1_name\n",
        "        file2_path = os.path.join(folder2_path, file2_name)\n",
        "\n",
        "        # Read the contents of file 2\n",
        "        with open(file2_path, 'r') as f:\n",
        "            file2_contents = f.read()\n",
        "\n",
        "        # Split the contents of file 1 into a list of words\n",
        "        words_to_replace = file1_contents.split('\\n')\n",
        "\n",
        "        # Loop over the words to replace\n",
        "        for word in words_to_replace:\n",
        "            # Replace the word in file 2 if it's present\n",
        "            file2_contents = file2_contents.replace(word, 'noun_tag')\n",
        "\n",
        "        # Write the modified contents of file 2 to the output folder\n",
        "        output_file_path = os.path.join(output_path, file2_name)\n",
        "        with open(output_file_path, 'w') as f:\n",
        "            f.write(file2_contents)\n"
      ],
      "metadata": {
        "id": "atL6AfMU27fZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Merging all dataset files into 1 file for both text and DRS.**\n"
      ],
      "metadata": {
        "id": "BDew-fY8QTmH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "merging text files into 1 file."
      ],
      "metadata": {
        "id": "llkcIv5aQ43_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Replace with the appropriate folder path\n",
        "folder_path = '/content/text_files_modified'\n",
        "\n",
        "# Replace with the appropriate output file path\n",
        "output_path = 'modified_text.txt'\n",
        "\n",
        "with open(output_path, 'w') as outfile:\n",
        "    for filename in os.listdir(folder_path):\n",
        "        filepath = os.path.join(folder_path, filename)\n",
        "        with open(filepath, 'r') as infile:\n",
        "            outfile.write(infile.read())\n",
        "            outfile.write('\\n')  # Add a newline character between each file\n"
      ],
      "metadata": {
        "id": "3RXZiYIVQnwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Merging DRS files into 1 file."
      ],
      "metadata": {
        "id": "V5ZUNeYwRiJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Replace with the appropriate folder path\n",
        "folder_path = '/content/drs_files_modified'\n",
        "\n",
        "# Replace with the appropriate output file path\n",
        "output_path = 'modified_drs.txt'\n",
        "\n",
        "with open(output_path, 'w') as outfile:\n",
        "    for filename in os.listdir(folder_path):\n",
        "        filepath = os.path.join(folder_path, filename)\n",
        "        with open(filepath, 'r') as infile:\n",
        "            outfile.write(infile.read())\n",
        "            outfile.write('\\n\\n')  # Add a newline character between each file\n"
      ],
      "metadata": {
        "id": "tjDNg6I1RowE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Trying SST using BookNLP**"
      ],
      "metadata": {
        "id": "JwKwowhOV2Z0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install booknlp\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "IEPXOFPbX-QO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from booknlp.booknlp import BookNLP\n",
        "\n",
        "# model_params={\n",
        "# \t\t\"pipeline\":\"entity,quote,supersense,event,coref\",\n",
        "# \t\t\"model\":\"big\"\n",
        "# \t}\n",
        "\n",
        "######## i am only interested in SuperSenses model parameters so i am using only SuperSense.\n",
        "model_params={\n",
        "\t\t\"pipeline\":\"entity,supersense\",\n",
        "\t\t\"model\":\"big\"\n",
        "\t}\n",
        "\n",
        "\n",
        "booknlp=BookNLP(\"en\", model_params)\n"
      ],
      "metadata": {
        "id": "PCxk8fHZYAvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Input file to process\n",
        "input_file=\"text.txt\"\n",
        "\n",
        "# Output directory to store resulting files in\n",
        "output_directory=\"text_output/\"\n",
        "\n",
        "# File within this directory will be named ${book_id}.entities, ${book_id}.tokens, etc.\n",
        "book_id=\"bartleby\"\n",
        "\n",
        "booknlp.process(input_file, output_directory, book_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGAv2jF2YC2U",
        "outputId": "47e49b66-d797-4ae9-8827-9f169403c392"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- spacy: 0.032 seconds ---\n",
            "--- entities: 0.504 seconds ---\n",
            "--- quotes: 0.000 seconds ---\n",
            "--- name coref: 0.000 seconds ---\n",
            "--- TOTAL (excl. startup): 4.828 seconds ---, 61 words\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "applying BookNLP on multiple text files."
      ],
      "metadata": {
        "id": "j7p0teLxdFU_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "#import booknlp\n",
        "\n",
        "# Define the input directory containing the text files to process\n",
        "input_directory = \"/content/dev_subfiles/\"\n",
        "\n",
        "# Define the output directory to store the resulting files\n",
        "output_directory = \"sst/\"\n",
        "\n",
        "book_id=\"bartleby\"\n",
        "\n",
        "# Loop over each file in the input directory\n",
        "for filename in os.listdir(input_directory):\n",
        "    # Check that the file is a text file\n",
        "    if filename.endswith(\".txt\"):\n",
        "        # Define the input file path\n",
        "        input_file = os.path.join(input_directory, filename)\n",
        "        # Define the book ID based on the filename (without the .txt extension)\n",
        "        book_id = os.path.splitext(filename)[0]\n",
        "        # Define the output directory for this book\n",
        "        book_output_directory = os.path.join(output_directory, book_id)\n",
        "        # Create the output directory if it doesn't exist\n",
        "        if not os.path.exists(book_output_directory):\n",
        "            os.makedirs(book_output_directory)\n",
        "        # Call the booknlp.process() function to process the input file and save the resulting files\n",
        "        booknlp.process(input_file, book_output_directory, book_id)\n"
      ],
      "metadata": {
        "id": "5YMzuZJ2V5gU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "working on small data examples, 10 only."
      ],
      "metadata": {
        "id": "e9eiNtYTof6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the supersense file into a pandas DataFrame\n",
        "supersense_file = 'text_output/bartleby.supersense'\n",
        "df = pd.read_csv(supersense_file, sep='\\t', usecols=[0, 1, 2])\n",
        "df.columns = ['start_token', 'end_token', 'supersense_category']\n"
      ],
      "metadata": {
        "id": "kvk0H7dXlD-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the original text file into a string variable\n",
        "with open('text.txt', 'r') as f:\n",
        "    text = f.read()\n"
      ],
      "metadata": {
        "id": "VyzvP6qvwE6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the supersense file into a pandas DataFrame\n",
        "supersense_file = 'text_output/bartleby.supersense'\n",
        "df = pd.read_csv(supersense_file, sep='\\t', usecols=[0, 1, 2, 3])\n",
        "df.columns = ['start_token', 'end_token', 'supersense_category', 'text']\n",
        "\n",
        "# Filter the DataFrame to only include rows where the supersense category starts with \"noun\"\n",
        "noun_df = df[df['supersense_category'].str.startswith('noun')]\n",
        "\n",
        "# Write the filtered DataFrame to a new file\n",
        "noun_df.to_csv('noun_supersenses.csv', sep='\\t', index=False)\n"
      ],
      "metadata": {
        "id": "PdVosoA6wHEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "transforming noun.tag --> noun_tag"
      ],
      "metadata": {
        "id": "0amtae5RWc0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the \"noun_supersenses.csv\" file into a pandas DataFrame\n",
        "df = pd.read_csv('noun_supersenses.csv', sep='\\t')\n",
        "\n",
        "# Replace any \".\" characters in the \"supersense_category\" column with \"_\" characters\n",
        "df['supersense_category'] = df['supersense_category'].str.replace('.', '_')\n",
        "\n",
        "# Save the modified DataFrame back to the \"noun_supersenses.csv\" file\n",
        "df.to_csv('noun_supersenses.csv', sep='\\t', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIQx5LzB9hjt",
        "outputId": "79222a31-b67a-481d-fa81-041813c91e46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-3871059852cd>:7: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
            "  df['supersense_category'] = df['supersense_category'].str.replace('.', '_')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "this is based on word matching."
      ],
      "metadata": {
        "id": "Y7TSoGPsH4ts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "# Load the CSV file into a dictionary\n",
        "with open('noun_supersenses.csv', 'r') as f:\n",
        "    reader = csv.DictReader(f, delimiter='\\t')\n",
        "    mapping = {row['text']: row['supersense_category'] for row in reader}\n",
        "\n",
        "# Load the text file and replace matching text with supersense category\n",
        "with open('text.txt', 'r') as f:\n",
        "    text = f.read()\n",
        "    for key, value in mapping.items():\n",
        "        text = text.replace(key, value)\n",
        "\n",
        "# Write the modified text back to the file\n",
        "with open('modified_text.txt', 'w') as f:\n",
        "    f.write(text)\n"
      ],
      "metadata": {
        "id": "KzdytemLAXEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqM5U0e8WkDp",
        "outputId": "216f98e3-46cd-4005-94f9-508ddb265ca7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drs.txt             modified_text.txt     \u001b[0m\u001b[01;34mtext_output\u001b[0m/  text.txt.raw\n",
            "modified_text2.txt  noun_supersenses.csv  text.txt\n"
          ]
        }
      ]
    }
  ]
}